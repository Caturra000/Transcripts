# 追捕海森堡错误

标题：Hunting Heisenbugs

日期：2023/11/14

作者：Paul McKenney

链接：[https://lpc.events/event/17/contributions/1504/](https://lpc.events/event/17/contributions/1504/)

注意：此为 **AI 翻译生成** 的中文转录稿，详细说明请参阅仓库中的 [README](/README.md) 文件。

-------

我们今天要讨论的是追捕**海森堡错误** (Heisenbugs)，这有点像是软件工程中的印象派风格。因为在印象派绘画和海森堡错误这两种情况下，你越近看，看到的反而越少。对于印象派绘画，我想那是其意图所在；而对于海森堡错误，这通常被认为是个问题。所以我们今天要讨论解决这个问题的方法。这目前仍然更偏向于一门艺术而非科学，但我们应该至少能获得一些可能对你们有帮助的技巧。

好的，我们将讨论几个主题。当然，最后还有一件非常重要的事情：追捕海森堡错误固然很好，但避免它们则更胜一筹。你们知道，海森堡错误在我生命中的某个时期，大约30年前，是可怕的、改变人生的经历。但过了一段时间，你多少就习惯了它们，所以到了星期二，对吧？我们今天有什么海森堡错误要对付呢？

大型机群实际上非常擅长检测海森堡错误。这是纯粹的数学问题，但有时会令人惊讶。如果我们有一个十万年才出现一次的Bug（十万年Bug），在一个百万台系统的机群上，那就是每百小时发生一次。所以，如果我们有一个万年Bug，在百万台系统的机群上，就是每百小时发生一次。我不被允许告诉你们我的雇主运行了多少台系统，但我可以告诉你们至少有一百万台。所以如果我们遇到了一个万年Bug，并且它是某种一致性的问题，比如一个竞争条件（原文为"worn on"，应为"race on"或"race condition"）... 我是说，如果是内存损坏，它每次的表现都不一样。好吧，你知道，那可能是硬件故障，我们无法区分。但如果它导致了某种竞争条件，我们就会看到它，并决定如何处理。我们可能会意识到，或者决定在当前版本中修复它的风险更大，所以我们可能只在上游做一个修复，让它稍后下来。或者我们决定要修复这个Bug，但我们会看到它并做出决定。

那么，我们如何检测它呢？**控制台输出**是一个常见的方法，就像在其他地方一样。当然，有很多自动化工具收集这些输出。你可以使用**BPF**（Berkeley Packet Filter）。你可以发布一个**内核补丁**来检测问题，但这非常痛苦，因为像Chris Mason这样的人会理所当然地就一个调试补丁被分发到那么多机器上向你提出许多棘手的问题。你也可以设置**内核调试器** (kernel debugger)。所以我要做的是，谈谈几周前Chris让我修复的一个Bug。这实际上非常受欢迎，我当时正在工作... 我不记得当时在做什么了，那件事非常令人沮丧。所以我就想，好的Chris，我会看看你的问题（来逃避我的问题）。这是一个在控制台有输出的Bug，当时已经是傍晚了。所以我看了看就睡觉了，醒来时就知道问题所在了。这当然是一个有趣的过程，如果修复的不是我自己的Bug，那就更有趣了，但你知道，你不可能事事如意。关键是，这个Bug的发生率是每周每4000台系统出现一次故障。这足以让我们决定：好吧，我们不再继续推广这个版本了。

嗯，那相当于单台系统的平均故障间隔时间 (MTBF - Mean Time Between Failures) 是70年。但那个Bug很容易修复。这是一个修复，所以我们可以立即发送出去。如果它是一个功能增强，就必须先提交上游，然后我们再拉下来，但修复补丁在道义上是可以接受的（立即发布）。我把它发出去后，Chris看了看说，嗯，我们看看。好吧，这是一个回归（regression），但70年的MTBF，我们可以等到下一个合并窗口（merge window），对吧？没问题，那没问题。

现在，你理论上可以设置内核调试器，但是... **一百万台**kgdb实例？我还没想出任何办法能让这看起来哪怕有一点点像是有趣的事。你知道，除非你有足够的受虐倾向，我猜... 但你知道，管理起来会非常痛苦。而且，如果你有一个修复在等待部署到足够多的机器上来检测问题，那也不会特别有趣。所以你真的需要一个比这更好的方法。当然，好方法已经存在一段时间了。我在这里做了很多总结。

现在你可能会问：等等，我没有大型机群。你在说什么？别烦我。你知道，我只有15台系统，我只有一千台系统。这是什么废话？嗯，你知道，在我拥有大型机群之前，我追捕海森堡错误已经很长时间了。所以也许你们现在的情况和我90年代初一样，当时... 我想Sequent公司（作者曾任职的公司）最多只有6000台机器，也许在其巅峰时期有10000台机器。但我们仍然需要追捕海森堡错误。另一件事是，我雇主的机群在全球Linux内核安装基数中只占相当小的一部分。一个叫Dave Rusling的家伙在2017年告诉我，当时有200亿个Linux内核实例在世界各地运行。嗯，能减少一些随机故障的东西总是好的。所以修复其中一些问题会非常好，因为你认为它们在百万台系统上发生很多次？试试100亿台吧，对吧？现在，嗯，硬件故障掩盖了其中许多问题，还有很多设备和其他嵌入式系统，只要发生任何不好的事情就会自行重启，所以你可能不会注意到。但还有一件事是，无论我们喜欢与否，无论我们赞成与否，Linux内核越来越多地用于安全关键应用，并且已经超过10年了。就我所知，我希望它没有用于像航空电子设备这样的领域。对于航空电子设备，他们有非常严格的编码风格指南。事实上，我最后一次接触这类事情时，如果你想加一个`if`语句，你将不得不花很多小时在会议上**论证**为什么你需要那个`if`语句，以及为什么不能用其他方式实现，因为那个`if`语句会增加程序的复杂性，使其更难以验证。内存分配？**想都别想**，你绝对不能做内存分配。一旦系统开始运行，在初始化阶段，飞机还在地面上时？好吧，也许可以，但其他时候绝对不行。关键是，在很多司法管辖区（我不知道这个地区的情况），但世界上其他一些司法管辖区，如果你有一个可能包含计算机并且可能运行Linux内核的机制，如果它在跨越若干年（我知道的一个案例中是5年）的测试中表现正常，它就不会被认证为安全关键。Linux内核可以轻松做到这一点。所以我们将长期看到Linux内核被用于低级的安全关键应用。嗯，我认为即使这些Bug发生的频率不高，为了安全关键应用，修复其中一些也是好的。

好了，那么这里的关键技术是什么？好吧，首先，为什么这被称为海森堡错误？我们为什么这么叫它？嗯，因为**当你添加调试时，它就消失了**，对吧？这差不多就是定义。嗯，原因通常是这个Bug与时间（timing）相关，如果你添加调试，你就改变了时间，可能掩盖了Bug。有时你添加调试反而可能让Bug更容易出现，但我们不像记住前一种情况那样清楚地记得这些。你知道，就是这样的事情。所以时间的微小变化可以减少（Bug）发生。处理这个问题的一种方法是**降低MTBF**，换句话说，**增加Bug发生的频率**。这在调试时感觉有点反直觉——你不想让Bug消失，但在这里，如果你让Bug发生得更频繁，你就可以添加调试而不会让Bug消失。对吧？所以，弄清楚它为什么会发生，然后让它更频繁地发生。好吧。那么，你怎么做到呢？这还有另一个动机，就我目前的工作而言，你希望能够在一小部分机器上运行一个小测试，比如在50台机器上运行一周的测试，并有某种把握，认为你已经消除了很多在百万台机器上才会出现的Bug。好吧，那会很方便。嗯，如果你做一下数学计算，那就意味着测试机器上的MTBF必须比生产机群上的MTBF小六个数量级。六个数量级！有趣的是，在很多情况下这很容易做到。并非总是如此，我的意思是，有时候生活是艰难的，很抱歉，这更偏向艺术而非科学。但在某些情况下，你可以做一些相当简单的事情，就能将MTBF降低许多个数量级，从而将海森堡错误转变为非海森堡错误。我们会讨论几种方法。

好的，我们到底怎么做？到目前为止我一直在做宣传。让我们来点真正的东西（干货）。我们要做的是利用我高中时的田径和越野教练的理念（那是在很久以前）。嗯，我们的训练课很艰苦。对吧？我们在越野跑中跑2.5英里的比赛，后来也变成了5000米（比赛）。你知道，某个法国公制系统来了。我不知道，不管怎样。关键是比赛日是轻松的日子。我们做的训练课比跑2.5英里或5公里要艰苦得多得多。我们在这里做同样的事情。这对计算机也适用。所以发生的事情之一是，既然我们很多人（许多听众）关注的是Linux内核，如果你去看看你系统上Linux内核的利用率... 我们接下来讨论内核的**非空闲利用率**。好吧，这个利用率高于10%是不常见的。我是说，我确信有些工作负载确实如此，别误会我。但在许多生产环境中，实际的内核执行时间——非空闲内核执行时间——是几个百分点的小数字，甚至可能低于1%。如果你和应用开发人员交谈，他们认为生活本该如此。那是上帝的旨意，因为他们想运行他们的应用，在他们看来，内核是税，他们不想付，能避免就避免。所以他们会努力让内核尽可能少运行。这对我们内核开发者来说真的很好，因为这意味着我们只需让一个测试持续运行内核，就能将MTBF降低一到两个数量级。

对吧？所以他们的应用以某种方式调用内核。如果我们能做一些类似的事情，但只运行内核，我们就能获得一个到两个数量级的改进。这就是`rcu torture`测试套件的主要优势之一。它是一个内核模块，它的用户空间部分只是一个目录和一个小的可执行文件，仅此而已，其余都在内核里。嗯，所以一个反海森堡错误（anti-Heisenbug）... 如果海森堡错误源自物理学领域，它实际上是观察者效应（observer effect），与量子物理之类的东西完全无关。它实际上只是经典物理学中的观察者效应，但这名字已经存在了，也固定下来了，而且也挺有趣的，为什么不呢？所以如果我们从物理学领域（无论是经典的观察者效应，还是真正的海森堡测不准原理）得到了海森堡错误... 好吧，反海森堡错误也是物理学给我们的。好的，其中一种方法是**增加工作负载强度**。我们增加工作负载的强度，其含义会因工作负载而异。但如果我们增加了强度，就会降低MTBF（增加故障率），我们可能会达到一个点，在那里我们实际上可以放入调试而不会让一切（Bug）消失。在追踪内核Bug的情况下，只需让机器只运行内核，猛烈地敲打它，让它完全围绕内核运行，你应该能获得一到两个数量级的改进。这有点像“在隔离环境中测试可疑子系统”的一种特例，这里可疑子系统是整个内核。但你可以做得比这更好。

你可以专注于内核的某些部分。有时你也可以配置应用程序，使其所做的工作被简化。也许它正在处理带有海量磁盘的大型数据库。好吧，我们可以让它使用内存文件系统并拥有一个非常小的数据库，也许那样就能行，对吧？然后你就能获得那些（简化带来的好处）。这时你失去了磁盘I/O。但同样，根据工作负载，你可以查看关键部分是什么，并尝试专注于它。一种方法是**获取应用程序的跟踪**（traces），然后编写一个工具，只针对内核运行这些跟踪，很可能比应用程序本身消耗的CPU时间少得多。这是增加强度的一种方式。另一种方法是运行比生产环境更多的CPU、更多的内存、做更多的I/O。有很多方法，取决于故障发生在哪里。你的故障是CPU相关的有趣事情？也许增加更多CPU，也许增加更多插槽。插槽会增加延迟。好吧。

所以关键是，你要问自己：过去什么导致了问题？这是回顾性的，我们稍后会谈到前瞻性的，但让我们从回顾这个简单的事情开始。让我们回顾过去导致问题的东西。你这里的反海森堡错误就是：**寻找并制造麻烦**（look for and promote trouble）。这又有点反直觉——你在调试，你希望事情正常工作，对吧？但要度过这个阶段，你必须反过来做，让它变得更糟。你把它搞得很糟糕，以至于你的调试实际上能为你做些事，对吧？

关于这一点还有一点：如果你能增加强度让它更频繁地发生，那么当你找到一个修复时，你就不需要运行那么长时间来获得某种信心，相信这个修复确实改变了行为，甚至可能真的修复了它。当然，从统计学上讲，不可能证明你修复了系统。但你可以获得一定程度的信心，认为你将故障发生率降低了若干数量级。毕竟这是现实世界，我能说什么呢？

现在你要小心一点。当你增加强度时，通常也会增加延迟的数量。调度延迟增加了，你可能会有更多的任务（如果你那样做了）。如果你增加了CPU数量，可能会有更多的锁争用（lock contention）、内存争用（memory contention）。你知道，会有这些情况出现。你可能会遇到软锁死（soft lockups）、RCU CPU停滞警告之类的东西。如果你正在追查一个Bug，让这些东西告诉你“顺便说一句，你把强度调得太高了”并不是你想要的。你可以禁用它们，然后只看你想看的。但让我们先看看这（增加强度）在做什么，因为如果我们正确看待它，它给了我们另一个反海森堡错误。

所以，嗯，这个理论（排队论）是极度抽象于现实的。我们假设一个M/M/1队列。我们假设到达是泊松过程（指数分布），无论队列中有多少人，新的到达都是随机的。服务器（处理单元）从队列中取出东西也是随机的，完全随机，也是指数分布。而且只有一个队列。嗯，队列可以是无限的，很长，除此之外，它是完全现实的。嗯，理论家喜欢它的原因在于，它有一个非常好的闭式解。我的意思是，用三页纸你几乎可以从头推导出来，这很美妙。如果你数学比我好，可能用更少的页数。嗯，但这就是你得到的。

所以，在底部（X轴）我们看的是利用率。系统有多忙？1表示我们把它推到了极限，不能再做了；0表示我们什么也没做。嗯，延迟（Y轴）是以你前面排队人数的形式表示的，对吧？如果是0，那里没有人，所以如果你来了，你就是队伍的第一个人，保证你前面有0个人。在远端（右边），如果你正被猛烈冲击，每个人都在你前面排队，除了那里有无限多的人，所以你知道这看起来有点像指数曲线。我们应该这么幸运（看到指数曲线）。你看到一个指数曲线直到无穷大才趋向无穷大，而这东西在1（利用率）时就趋向无穷大了。那东西（曲线）径直上升，你知道的。是的，类似那样，它是... 嗯，利用率除以（1 - 利用率），所以是的，它直接上升到那里。好吧，嗯。

是的，所以在满利用率时延迟会爆炸。这有点不切实际。嗯，这是理论，所以我们可以做得更好一点。我们可以在末尾加一个K，说我们的队列最多只能有一定长度。如果有人来时队列满了，我们就把他们扔掉。这有点像医院，你只有那么多床位。抱歉，我很抱歉，但世界就是这样运行的。我没说你必须喜欢它。嗯，通常他们会转到另一家医院，但有时他们... 我不知道他们做什么，但无论如何，这就是这个模型的来源。这是导致这个模型产生的现实世界事物。这一个（有限队列模型）更烦人一点，它有一种闭式解，但这是你最终得到的结果。现在我们有了无限队列模型（的曲线）在那里显示着，上升到无处（无穷大）。但关键是，如果你的队列中只有有限数量的元素，如果你足够幸运能进入队列... 这是这个问题的情况。我的意思是，如果你进入了队列，你最多要等待接近队列长度的（最大）时间。

在这里（高利用率端），当然，你实际进入队列的概率是相当低的，正如一些在错误时间患上健康问题的人可以证明的那样。所以，嗯，它变得平坦了，但同样有趣的是它仍然（在高利用率时延迟很高）... 你可能需要调整你的超时（timeouts），因为10倍于正常延迟可能会引起问题。但这是你可以做的事情，对吧？你可以调整超时，你可以禁用RCU CPU停滞警告。有一个sysfs的东西可以做到这一点，等等。只是要意识到你可能不得不这样做。

另一件你可以做的事情是策略性地注入延迟。关键部分在于... 我之前在谈论强化工作负载的某一部分，但那可能不可行。我的意思是，它可能已经是你所能达到的最大强度了。在这种情况下，有时你可以通过*降低*另一部分的强度来获得结果。如果是一个竞态条件，你也许可以通过让竞态条件中脆弱的部分花费更长时间，来让它更频繁地发生。延迟是个美妙的东西。嗯。

嗯。现在，嗯，一些例子。我之前提到过多插槽。如果你在生产中通常在单插槽系统上运行（我的雇主主要是单插槽系统），那么在你的测试中运行在多插槽系统上会注入延迟。你会发现更多的Bug，并有更好的机会在生产环境中运行得更好。`rcu torture`实际上会在一些场景中注入延迟，以触发过去曾带来麻烦的竞态条件。在90年代的老日子里，我们只是拿不同速度的CPU插在同一个总线上。对于某些类型的竞态条件，时钟频率比能显著增加Bug发生的概率。我们当时能达到大约3:1的时钟比。今天你应该能够直接做到这一点，尽管... 系统倾向于从你手中夺走控制权，让时钟变成它们想要的样子，而不管你在做什么。有时你需要花点功夫才能让它工作，而且自动化也很困难。所以我倾向于避免它。另外，客户机操作系统... 也许有办法在客户机操作系统上做，但我还没找到。

所以关于RCU的关键点是：RCU必须等待CPU到达某些地方，在某些情况下也等待线程。但如果CPU离线了，RCU就不必等待它。如果CPU在其宽限期开始后才上线，它也不必等待它。对吧？但没关系。我的意思是，只要它做了该做的事，即使它等了一个它不必等的CPU，那也没问题。关键是RCU有一个组合树（combining tree）数据结构，这使它能够减少阻塞连接，同时还能看到全局状态变化。RCU在整个结构中是否等待一个CPU（上下线），保持一致性非常重要。如果上层认为它在等待一个CPU，而下层不这么认为，那将是非常糟糕的。在这种情况下，下层永远不会报告该CPU做了任何事，你将在那个宽限期上永远等待下去，那会令人沮丧——相信我的话，或者你可以自己试试让它发生。

所以正常情况下的样子... 我在这里做了缩写。`ab_show_supply_buffered_cpu_offline`是CPU离线热插拔操作，`ibcwe`是`initialized cpu waiting bitmask`（已初始化CPU等待位掩码）。通常这些操作发生得非常快。嗯，它只是遍历每16个CPU，获取锁，然后对CPU做点什么，再去CPU再做点什么，整个过程可能在10微秒内完成，可能比这还快。而CPU热插拔操作通常需要几百毫秒，甚至几秒钟。所以通常竞态条件根本不会发生。因此，有一些方法会在初始化的这两个阶段之间插入一个大的延迟，以强制CPU热插拔操作在那段时间内完成，这样我们就能看到问题。暴露这些问题的Bug。所以，策略性地插入延迟是一种方法。这里的问题是你可能会反对说：嗯，我必须知道在哪里放延迟，所以我得知道Bug在哪里。这是真的。

但很多时候，你知道，你写了那段并发代码，你多少有些想法，你可能会对自己承认... 你知道，你可能真的在否认，但你大概知道在哪个部分你有点累了不想再继续了，或者那个你觉得“嗯，我想这样应该能行”的部分... 你知道，那就是你放延迟的地方，对吧？或者你尝试向别人解释时，那个你得说得非常复杂的地方，他们一脸茫然地问“你在说什么？”——把延迟放那里！好吧。所以这很直接。嗯，呃，它并不总是有效，但这是一个很好的启发式方法。

统计未遂事件。我们又要回到航空公司了。我们之前谈过它们的安全关键性。美国联邦航空管理局要求报告未遂事件，如果两架飞机在飞行中过于接近。两架飞机的飞行员都必须提交一份关于该事件的正式报告，并进行调查。这样做的原因是，未遂事件是无意的，可能表明存在某种问题导致了它的发生。顺便说一句，这边有空的椅子，如果你们站累了，这里也有一些地方（可以坐）。但如果你喜欢站着，别让我妨碍你。嗯，呃，嗯，关键是，未遂事件可能指示了一个问题，并且这次问题没有导致碰撞，但最好在它导致碰撞之前修复这个问题，你知道... 软件也可以做同样的事情。如果你能找到一些迹象表明曾有一次险情（close call），那么这意味着你可以更快地评估提交、配置等，如果你在二分查找。这意味着你可能可以运行更短的测试，使二分查找更快完成。特别是如果你有一个声称的修复，并且你依赖未遂事件计数而不是实际问题计数，你可能可以花少得多的时间来验证，并且仍然有一定信心认为你将Bug发生的MTBF降低了某个数量级。

在RCU中，我们有一个未遂事件计数器。原来我们通常拥有的东西... `rcu torture`不调用RCU（原文指`rcu_torture`测试模块模拟的读者行为）。然后当它得到回调时，在很远的地方才调用。它说，好吧，嗯，状态改变了，对吧？所以我们设置... 我们增加一个计数器。然后读者如果看到计数器推进得太远，会说，嗯，我们有问题了。问题是，错误的... 错误读者检测到错误... 我的意思是，我想RCU读者本身并不是错误的，是在检测错误。但你知道，有时你得怪信使，你知道，有时你得干掉那个信使。嗯，呃，在这种情况下，我们必须有那个完整的跨度（时间间隔）才能检测到它。计数器有一个值，必须等到很远的地方（很久之后）才能得到另一个值。

现在我们保留一个计数器，当我们启动一个宽限期时，它会记录宽限期的开始和停止。如果我们同时也对这个计数器进行采样，那么我们就能检测到一个未遂事件。一个正确的RCU读者是不会看到任何未遂事件的。问题是，宽限期计数器的停止没有同步，上面没有内存屏障。所以我们可能会看到一个假阳性（false positive）的未遂事件。`rcu torture`处理这个问题的方法是，如果在一个运行中看到一个未遂事件，它说，嗯，无所谓。但如果它看到一堆未遂事件，它就说，嗯，我们可能有问题了。嗯，所以，呃，在这种情况下，很酷的一点是，未遂事件比实际Bug要频繁大约两个数量级，当我们有问题时。这意味着测试运行时间可以短得多，而且这是在`torture`通过持续运行内核获得一到两个数量级改进之上的。所以这还没有达到六个数量级，但我们大概达到了三或四个，那已经很接近了。嗯，所以当你的软件有迹象表明事情接近出错时，统计未遂事件是一个很好的反海森堡错误方法。

好了，我们已经讨论了如何让罕见事件频繁发生。我们看了利用率。我们将浏览其他几种方法，最后再看看核选项。

好的，这是我们之前看到的同一个队列，有限队列那个，没有爆炸到无穷大，只是达到了某个值（K）。我们有一个状态转换图，显示我们正从一个状态转换到另一个状态，队列长度在增长。在远端（左边），队列中有0个元素；在这一端（右边），它有K个元素。我们之前称K为10，我们再使用10。Lambda (λ) 是到达率（增加元素的速率），Mu (μ) 是服务率（移除元素的速率）。为什么费这个劲？因为我们要绘制处于这些状态的概率。所以现在你知道这些状态是什么了。

我们选择K=10。嗯，现在如果我们有利用率等于0.1（10%利用率），这对于生产环境的内核来说已经是高利用率了，对吧？如果在这个队列上是这样，那么队列中有10个元素的概率是10的负10次方（10^{-10}），非常小。如果我们将利用率提高到90%（0.9），我们在那个状态（队列满）的概率增加了九个数量级。好的，这有点理论化，但效果是真实的。现在内核中有多少个队列？嗯，网络（子系统）的伙计们有很多。我有一些用于回调（callbacks）的，但RCU并不真正关心队列里有多少回调... 但它不是唯一的东西。所以再次强调，一个反海森堡错误... 这里有点停顿，海森堡错误是被队列淹没... 但它不仅仅是队列，这适用于比我展示的队列更广泛的情况。

让我们回到1993年。嗯，我们当时正在重做内存分配器。嗯，这是一个简单的东西。它是32位的。你会惊讶于在30位系统上你能用内存分配器干多少坏事，但今天它完全行不通了。好吧，但我们当时充分利用了它。它也是类似的。我们有每CPU缓存，我们想让它工作得更“辛苦”，以确保你通常只去每CPU缓存。那时和现在一样，我们有一个全局缓。当我们在90年代中期加入NUMA时，我们也有了每节点缓存（per node caches）。然后，最终，你会把块放回页中。最终，你能够把这些页放入一个2兆的区域，然后释放物理内存，最终你实际上可以释放那个2兆的虚拟地址范围。我们非常努力地确保系统大部分时间停留在（缓存的）那边（左侧），非常努力。好吧。

这实际上类似于那个队列结构，对吧？你处于那边（缓存命中）的概率非常高，而处于这边（需要复杂操作）的概率很低。你在状态间移动... 我的意思是，内存分配器的状态空间要复杂得多，东西在四处移动，但如果你想，你可以把它投影到这个更简单的图上。结果证明这在当时是有用的，或者说，如果我在90年代中期意识到这一点，它本应是有用的。

所以，嗯，我们在90年代中期（在Sequent）的一大重点是用于高可用性的共享磁盘。所以我们会有两台服务器，用光纤通道连接，这样每台都能访问SCSI磁盘链。内存分配器在那里发挥作用是因为我们正在做一个分布式锁管理器，用于某个数据库，这样我们就可以运行分布式数据库。好处是，如果你丢失了一台数据库服务器，你仍然可以访问所有数据。当然，如果你有这样的东西，有节点会宕机，你会希望经常测试故障模式，否则当需要时它就不工作了。但出于某种原因，我们的一位客户反对这每晚发生在他们的系统上，对吧？幸运的是，它是在备份完成后发生的，而不是之前。好吧。那我们做了什么？我们给他们发了一个调试内核，对吧？他们对此很满意，因为它抑制了错误。我们有点担心它可能会在其他地方或其他客户那里出现，也许是在备份之前而不是之后，所以我们想最好还是看看。

所以，当然，嗯，那时和现在一样，你会发送一个崩溃转储，对吧？你们发行版的伙计们（对听众中发行版维护者说），你们仍然会从一些用户那里收到崩溃转储，对吧？有时也许... 好吧，也许不是。无论如何，那是我们当时做的。问题是这个崩溃转储完全是一场灾难。我的意思是，通常如果你得到一个堆栈跟踪，你会期望几件事：你期望堆栈跟踪中实际有一些函数名，你还希望堆栈中一个点的函数调用另一个点的函数，因为堆栈上的函数应该有调用关系。这些崩溃转储没有这个属性。嗯，它简直就是一场灾难，就像，你知道，“我不确定这里发生了什么，但它不是我们想要的”。

所以我们做的是，我们尽可能复制他们的工作负载。由于各种法律原因，他们不能直接把工作负载给我们。所以我们不得不根据他们的描述尽可能模拟它。最终，几周后，我们达到了可以在运行这个模拟5到27小时内重现问题的程度。他们设法让它每晚都发生（原文"like that"指重现），而我们却需要5到27小时。这里的一个问题是，测试一个声称的修复必须花很长时间，所以它（测试修复）花的时间是（重现的）很多倍。它花了很多很多周才完成。

嗯，最后发生的是，我们从这个东西上得到了很多堆栈跟踪，这些线条非常清晰（指幻灯片上的图）。我显然需要升级这个——箭头头部有向后延伸的线，难以置信（指箭头画得不好）。最终我们得到了一个堆栈跟踪，里面有两个函数，其中一个函数调用了另一个。是的，耶，对了！嗯，不幸的是，我发现是我的代码（的问题）。不能事事如意，你知道。这是根据记忆重建的。有几个原因让它成为记忆重建：出于法律原因，我仍然不被允许查看那段代码——记得2003年事件的人会知道为什么。不管怎样，这是那种32位下内存稀缺的情况。

所以你没有把2兆字节的区域对齐在2兆字节边界上。所以你有这样的情况，那里可能有一个小的黄色箭头（指针）指向6MB到8MB之间。如果你用2兆去除它，你会得到数组中错误的元素——每个内存区域对应一个数组元素。这没什么大不了的，你看看指针，如果指针比你的区域大，你就去找前一个元素，就像那边的代码说的。你知道，很直接。不幸的是，嗯，那时的编译器不像现在这么激进，但80386没有那么多的寄存器，编译器会说：“嘿，我的寄存器满了，算了吧，我想把那东西（变量）取两次。谢谢。”是的，哦，是的，你会（遇到问题）。所以它会取一次，然后说：“哦，它是非零的。所以我去看那个`if`语句的另一部分。现在它（值）是0，对吧？”哦，那就没那么好了。我（的代码）在正确的地方吗？嗯，在那之后生活就急转直下，它完全导致了整个内存被破坏，这就是为什么我们的崩溃转储没有提供任何有用信息的原因。

在Linux内核中修复这个很简单：让那个变量只读一次（`volatile` 或者 `READ_ONCE`）。“Milo（可能指某人），把你的手从这东西上拿开，你不知道你在说什么。”嗯，问题是，嗯，我们只是给那个变量加了一个`volatile`标签，但是，嗯，这是一个罕见事件。技巧是让重现器（reproducer）让那个（错误路径）一直发生。换句话说，杀掉快速路径（kill the fast path）。嗯，做起来不是太难。我写了一个焦点测试（focus test），可能是我写过的最复杂的并行代码之一。但你知道，它把重现时间降低到了大约每12分钟发生一次（MTBF=12分钟）。所以当时我们有三种情况：

1. **现有测试**：没有发现这个问题，所以根据过去的经验，MTBF是无限的（在大量系统上）。
2. **基于用户工作负载的压力测试**：MTBF是5到27小时。它基于客户的工作负载，需要几台大型系统，花了几个星期才搭建起来。可移植性中等。我的意思是，它是那个工作负载，但它有I/O，有调度器，有一大堆东西在运行。它也给了系统很大的负载。
3. **焦点测试**：MTBF是12分钟（不到12分钟，我没费心做统计，我观察到的最长时间是十多分钟，所以我说12分钟）。我必须确切知道Bug是什么才能写出这个测试。硬件需求很小，只需要几颗CPU和很少的内存。我花了一两天时间写这个测试。可移植性可能很窄（适用范围有限），谁知道呢，它可能会引发其他Bug。但它明显主要不涉及I/O，因为没有I/O发生；也不涉及用户空间，没有用户空间运行。但影响是，我在运行测试时系统基本不可用。所以这里有一个权衡，你必须明智地选择。

但最大的问题是... 我之前说过我们在看过去什么导致了问题，这是回顾性的。关于未来我们该怎么做？嗯，你知道，要问自己的一个问题是：你的快速路径（fast paths）是否在向你隐藏Bug？答案是“是的”。好吧，如果你想被冒犯，你可以被冒犯。也许你被冒犯了才会去修复Bug，那很好。嗯，你的快速路径在向你隐藏Bug。所以测试中的一件事就是让那些快速路径不发生。当然你可能会反对说：“那性能和可扩展性怎么办？我为什么要那样做？”答案是：糟糕的事情（会发生）。好吧，但这是一个测试。此外，你有一些变通方法；你可以在更小的系统上运行。我为RCU测试做的一些事情，Jan Horn说过好话。我不知道他是否用它发现了任何Bug，但他对此表示赞赏。嗯，其中一个原因就是限制为4个CPU。在某些情况下，你可以接受大量的争用——这只是测试，你不在乎；你让系统运行，最终你杀死它，那就行了。另一件你能做、但令人惊讶的事情是：在新高度集成的系统上运行为旧系统开发的代码。这样做的原因是：2008年我使用的系统有4个插槽，16个CPU，比较交换延迟大约是100纳秒。你知道，一次单播（single cast）大约需要100纳秒才能让一个缓存行从一个CPU到另一个CPU再回来。2017年，一个插槽里有56个CPU的系统，在超过三倍的CPU数量下，给了我大约相同的延迟。当你想到这一点时，这并不坏。2017年，同一个系统，如果你在同一块主板上使用4个插槽，延迟大约是400纳秒。今天我试用了一个系统，对于相同数量的CPU，它只有两个插槽，延迟只有150纳秒——相同CPU数量下延迟几乎好了三倍。所以很可能我们10年或20年前写的快速路径在今天并不是真正需要的，或者没那么需要了。同样，2007年可能杀死你的争用，今天至少在测试系统中可能是可以容忍的。这个论点的另一面是系统变得越来越大，所以我们可能因此吃亏。更新的系统能更好地处理争用，所以你可能能够通过禁用你的快速路径来蒙混过关，而不会发生坏事。

我时间不够了，所以跳过那部分（指幻灯片）。还有很多其他可能发生的罕见事件，嗯，我就不赘述了。但这里的关键点，重点在于：如果你让一个罕见事件更频繁地发生，你会获得MTBF的某种降低（若干数量级）。如果你让两个（罕见事件）更频繁地发生，效果是乘性的。所以如果你做了两组，每组降低三个数量级，理论上你可能获得六个数量级的降低。我的意思是，在现实中，Bug想干嘛就干嘛。好吧，但统计上它们有时在某些方面表现得很好。这是一件值得尝试并且可能带来有用结果的事情。所以一个反海森堡错误就是：在你的测试中，组合那些罕见事件，让它们一直发生。如果你必须选择——外面有很多罕见事件，对吧？——选择过去造成最多麻烦的那些，或者最近开发最多的那些，是另一个选择。

**检测后插装（Detect then instrument）**。你遇到的一个问题是插装（instrumentation）改变了时间。一个技巧是，如果你有像RCU CPU停滞警告这样的东西能检测到问题，就把插装代码放在RCU停滞警告的开始部分。理论上，这不会影响Bug的MTBF。我的意思是，是的，你可能有代码大小的变化，它仍然可能有影响，但你至少有了一些机会，对吧？所以如果我们看看... 如果我们展开宽限期的全貌（的示意图），我们初始化宽限期，然后反复执行强制静止状态操作（force quiescent state operation），它寻找空闲的CPU，偶尔会敲打（唤醒）CPU来帮忙（报告静止状态）。然后过一会儿我们完成了，我们清理宽限期之后的事情，然后我们开始另一个宽限期，或者不开始，取决于是否需要。只是几年前，我们卡在那里了。它会运行下去，嗯，任务在等待，没有被唤醒。它有一个三秒的定时器？三毫秒的定时器？嗯，定时器没有触发，怎么回事？嗯，这（重现）花了数百小时，我不知道MTBF是多少。我知道它至少是几百小时的高位。嗯，如果我加上调试，它就消失了。嗯，所以我花了一年时间到处看，做配置更改，摆弄东西，更改启动参数等等。但我把MTBF降到了300小时，这算是合理了，但（问题是）如果我加上调试，它还是完全消失。关键是检测到它的RCU停滞警告代码本身... 突然它（系统）变好了，继续运行下去了。实际发出警告的代码踢了一脚，让它（系统）继续了。

所以技巧是再次把调试代码放在RCU停滞警告的最开始部分，然后我们就可以转储状态并找到Bug。在这个案例中，嗯，我们做的是：我们说，嘿，这个定时器，你知道，它在... 嗯，关于可中断的超时。嗯，`schedule_timeout_interruptible`，对了。我可能某天能说对（函数名），但那时可能没说对，你可以查一下。基本上，如果定时器花了至少8秒，并且比它应该花的时间长三倍，我们就说：好吧，我们去转储状态。嗯，这显然是启发式的，但在这个案例中它足够好了。这个Bug是由于定时器、CPU热插拔和RCU之间的交互引起的。我们现在就有一个（Bug）在发生，可能是在工作队列（work queues）和RCU之间，也许别人在用工作队列，我们还不太确定。但我们使用了同样的技术（检测后插装）... 工作（队列）今天？哦，不是今天，是几天前。好的。所以关键是，你在事后（检测到错误后）添加调试，这不会增加你的MTBF（不会减少Bug发生率）。所以一个反海森堡错误是：与其直接把调试代码放进系统，不如有某种方法检测到错误，*然后*在那时插入调试。崩溃转储是这种方法的一个特例，一个非常重要的用途。至于白盒测试... 我打算跳过那部分（幻灯片）。嗯，但，嗯，让我们检查一下你的记忆力。比精通追捕海森堡错误更好的是什么？

很好，如果有人记得或者能推断出来。是的，首先就不要有海森堡错误。嗯，有一些方法可以做到。嗯，它们并不是... 这有点像“吃你的蔬菜”那种东西。好吧，但你知道，吃你的蔬菜，你会感觉更好。嗯，所以有一些方法可以避免，但这永远不会完美。好吧，你可以拼命去做，仍然会有一些（Bug），但你知道，消灭一半也应该是值得的，对吧？嗯，所以另一个反海森堡错误是：不要随意修改并发代码，好吗？我的意思是，如果你真想这么做，好吧，但接受后果。好吧。

所以，我们谈了一些事情。嗯，没有银弹。这不是一门科学，但有很多有用的技术，希望其中一些能帮助你们。它们在过去的很多时候帮助过我。到此为止，嗯，我们还有点时间提问。是的，那边有一位。在（提问的）那位先生溜走之前把话筒扔给他。给他两个话筒，看看我们能不能接住两个。所以（第一个问题）你提到并发优先设计（concurrency first design），有哪些关于这方面的好资源可以阅读？特别是针对操作系统的。嗯，嗯，LKML（Linux内核邮件列表）有很多... 一些措辞相当... 尖刻的... 关于如何做的讨论。哦，你旁边那位举手了。你为什么不把话筒给他？也许他有好答案。（第二个问题）TLA+，所以Leslie Lamport的TLA+实际上被用来验证（用于修复）那个臭名昭著的GLIBC Pthread条件变量并发Bug。让我快速总结一下。是的，基本上，如果形式化验证适用于你的问题，嗯，你可以做一个简化版本。是的，所以如果适用，形式化验证。是的，好吧。而且它效果很好。也有Lynch和Vaandrager的I/O自动机模型，它有一些相同的问题。它功能弱一些，但更容易使用。好吧，嗯，你可以直接把C代码放进去。TLA+你需要多费点功夫，但另一方面，Cute spin locks（指某个具体的自旋锁实现）和Lineage journal实际上用TLA+验证过，他们发现并因此修复了一些Bug。嗯，但另一件事就是理解并发的东西是什么，嗯，遵循设计等等。好的，其他问题？顺便说一句，这是个好问题，而且... 嗯，是的。

所以（下一个问题），嗯，当你在追捕一个海森堡错误时，它是一个非常罕见的事件。当你开始添加插装或其他东西，或者增加负载时，你怎么知道你仍然在追捕同一个Bug？是的，你可能不知道。是的，然后... 小心点。这是现实世界。而且我认为我们提到了最初的情况... 不是，最初是每周每4000台机器发生一次（故障）。是的。你怎么知道它不是阿尔法粒子（alpha particles）引起的？因为阿尔法粒子不会总是在这行特定的代码上说“竞争条件发生一次”。但你是对的，如果你有一个海森堡错误，它只是一个像内存损坏的错误，要确定它是同一个会非常困难。嗯，但再次强调，没有银弹。哦，是的，嗯，是的。所有的消费设备（？提问者提到，但未记录完整问题）... 更多问题？

所以（下一个问题），在我参与内核开发之前，在Rust世界里，我们开发了一个很好的库。我们有一段并发代码，我们有一个库，你只需用Rust写一个使用你代码的测试，然后它运行所有排列组合来看是否能找到任何Bug。是的。我们在内核里有类似的东西吗？嗯，内核里没有像那样的东西。但是有一个有趣的数据结构用来跟踪RCU回调。我把那段代码拉到了用户空间，你知道，为我调用的所有内核函数填上桩（stubs），然后根据队列的状态进行详尽的测试。我会运行这些函数并验证它们是否按我预期的方式输出。嗯，但我们应该比你提到的更频繁地使用白盒测试。我在Kernel Recipes上和某人交谈过——我希望我记得名字——他们正在为一个Bug做完全一样的事情。他们有一个状态是合法的但罕见，Bug会在那个状态之后发生，所以他们作弊了：将数据结构初始化为那个（罕见）状态，然后让Bug发生。是的，白盒测试是一个极好的技术。是的，特别是如果你能把它融入语言本身，或者至少在语言中运行代码，而不必... 这不会对我们有任何隐性广告吧？抱歉，我忍不住（调侃Rust）。

我相信你可以在其他语言中做同样的事情。只是可能... 问题在于是否有人做过。在非常... 在某些方面。是的。其他问题？我想我们大概... 是的，我占用了一点下一位演讲者的时间。我为此道歉。谢谢。
