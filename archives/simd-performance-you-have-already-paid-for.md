# SIMD：你已经为之买单的性能

标题：SIMD. Производительность, за которую вы уже заплатили

日期：2021/04/30

作者：Ашот Варданян

链接：[https://www.youtube.com/watch?v=6Sh9QWdzo58](https://www.youtube.com/watch?v=6Sh9QWdzo58)

注意：此为 **AI 翻译生成** 的中文转录稿，详细说明请参阅仓库中的 [README](/README.md) 文件。

备注一：这是 StringZilla 作者的演讲，我最近在看这个库用到的算法（看不看得懂是另外一回事）。

备注二：不过这份演讲是比较偏向闲聊的，没有说到具体算法。

-------


**主持人**：大家好。  
**Ashot**：大家好。  
**主持人**：欢迎各位。让我们稍微聊一聊这次的演讲。你能否讲讲，为什么你决定做一个关于 SIMD 的演讲？比如，为什么不是关于 ARM 的 Intrinsics（内联函数）？

**Ashot**：其实 ARM 的 Intrinsics 也会在这个演讲中提到。SIMD 是一个通用概念，意思是“单指令多数据”（Single Instruction Multiple Data）。这并不意味着它只适用于 x86 架构。ARM 也有自己的 Intrinsics，我们稍后会讨论。

至于我为什么做这个演讲？因为我的工作围绕着处理海量数据的任务展开。通常我没有足够的资源去云端购买 10 倍的实例，所以我必须大力优化代码。起初我做的是关于显卡（GPU）计算的演讲，去年我就讲过这类话题。但并非所有计算都适合在显卡上高效完成。有些计算最好在处理器（CPU）上完成，无需将数据传输到其他地方。目前的演讲正是关于如何在无法将计算转移到其他设备时，加速处理器上的计算。

**主持人**：我很好奇，演讲中是否会介绍如何提示编译器在哪里生成那些 SIMD 指令的技术？

**Ashot**：会有这样的插曲，但在演讲过程中我们会看到，现代编译器并没有我们想象的那么聪明。甚至可以说，即使你给了它正确的提示，它就像黑板前的差生一样，回头看着班里的同学，试图搞懂谁在给它提示，但往往得不出正确的答案。所以最好还是要像优等生一样，亲自动手来做。

**主持人**：明白了。非常有趣，实际上非常令人期待。那么，如果你不介意的话，我们开始吧。

**Ashot**：好的，当然，我很乐意。

其实，我都不知道该怎么给这个演讲起个正确的标题。我觉得更有趣、更贴切的方式是聊聊天，讲讲关于低级编程的一些硬核故事。在会议开始时，已经有一个简短的演讲介绍了什么是 SIMD 指令以及如何做一些基础的事情。我们会在开始时讨论类似的基础内容，然后过渡到行业趣闻、特点以及如果你想在生产环境中使用这些技术最好要了解的事情。

首先，我是谁，为什么我在这里。我是一名程序员，用 C++ 和其他几种语言写代码。我写代码很久了，从小学就开始了。在高中时，我发布了几个相当成功的 IT 项目。后来我有几年投身于天体物理学，但两次都放弃了。而在过去的五年里，我一直致力于开发自己的框架。它叫 Unum。不幸的是，这不是一个开源框架，但它解决的任务类似于 TensorFlow 和 PyTorch，与机器学习有关，但意义更广泛。

在做这些的过程中，我周游世界很久，寻找有趣的程序员和开发者，并不断爱上各种新型计算：模拟计算、电子计算、光学计算、量子计算。我很乐意讨论所有这些，甚至可以在讲座结束后对比一下它们。但这是 C++ 会议，所以我们将讨论你们为何而来：也就是如何让解决你任务的代码稍微快一点，或者快很多。

对于完全属于我工作范畴的复杂任务，进行比较是没有意义的，所以我写了一个微基准测试（microbenchmark），展示如何使用 SIMD 指令和 Intrinsics 在普通硬件（比如你的笔记本电脑，甚至可能是手机）上解决“子串查找”（substring search）任务。

如果简单地表述子串查找任务：给你一个长字符串，称为 haystack（干草堆/大海），再给你一个小字符串 needle（针）。你需要在大海里捞针。

如果用 Python 编写与我们要写的 C++ 代码逻辑类似的代码，搜索速度大约是 **10 MB/秒**。也就是说它每秒可以处理 10 MB 的干草堆来寻找你输入的字符串。

C++ 的情况要好得多，因为与 Python 不同，我们有 `char` 数据类型，不需要单独分配数组元素，我们可以以 **2 GB/秒** 的速度遍历数组。

使用 SIMD 指令，我们可以将这个指标提升到 **12 GB/秒**。正如我们将看到的，这基本上是我这台架构的理论上限。因为这里只有 4 通道内存，对于单核来说，一个内存通道无法获得超过 12 GB/秒的速度。

如果你想进去看看这些基准测试，并正确地查看演示文稿，GitHub 上既有演示文稿也有基准测试代码。这就是链接，也已经发到了 Telegram 聊天群里。

那么，我们今天聊什么？左边是我们会视而不见、忽略的内容。我希望你们已经大概知道什么是 SIMD，什么是并行性（毕竟已经有相关讲座了），什么是无分支编程（branchless coding），以及分支预测（branch prediction）是昂贵的——我们将试图绕过这些。

接下来是一串算法列表。这不只是每行两三个名字那么简单。这实际上是子串查找算法。那些接受过正规计算机科学教育的人可能在课程中见过这些名字。实际上，人们期望这些算法的渐进复杂度能够胜过简单的暴力（brute force）搜索方法，但实际上，针对特定的处理器架构对它们进行优化要困难得多。

根据我的观察，无论是 Boyer-Moore-Horspool、Pratt 甚至 Rabin-Karp 算法，要么更慢，要么需要大量的预处理才能进行搜索。因此，我们将讨论暴力破解方法，什么是 x86 汇编中的 AVX2 指令，什么是推测执行（speculative execution），ARM 与 x86 有何不同，围绕 AVX-512 有哪些恐惧、恐怖故事和神话，什么是 L1、L2，但不是缓存（Cache）。我是指，我假设大家都听说过处理器的 L1、L2、L3 缓存，而我们要谈论的是 **L0、L1、L2 许可证（licenses）和频率**。这是完全不同的东西，尽管出自同一家制造商。我们将讨论有什么工具（tooling）以及我对未来的建议。

但在那之前，先给出几个定义是明智的，以免混淆。

当你写代码解决新问题、设计算法时，第一阶段是**伪代码（Pseudocode）**。你试图描述需要对某些抽象对象执行哪些操作。你还没有完整的函数调用，可能还没有一致的变量命名，你只是在描述逻辑。在这个层面上，我们将操作称为**助记符（Mnemonics）**。助记符就像一种语义规则，用于记忆你想做什么：加、比较、乘或执行逻辑操作如 AND 或 OR。

下一阶段，当你开始编写代码时，你可以直接用普通的 C++ 写，并使用**内联函数（Intrinsics）**。Intrinsics 就是那些嵌入在编译器中的类 C 函数，你可以在代码中调用它们。它们通常有如下签名：首先，如果是 x86，通常是 `_mm`，后面跟 `256` 或 `512` 或什么都没有。这定义了操作区域的大小。然后是助记符本身，比如 `cmp_eq`，即 compare equality（比较相等）。`epi32` 是数据类型的大小，我们要将其映射到 256 位寄存器上。也就是说，我们需要成对比较 256 位寄存器中的所有 32 位部分。结果我们将得到 8 个掩码。具体到这个操作，只有两种类型——全 0 或全 1。

然后我们希望编译器根据我们要运行的架构，为我们的 Intrinsics 推导出正确的**指令（Instructions）**。到了指令阶段，我们的代码在反汇编器（disassembler）中看起来是这样的：有指令名称和寄存器指示。比如最后两个寄存器——`%ymm0` 和 `%ymm4`——分别是 256 位寄存器列表中的第 0 号和第 4 号寄存器。`Y` 寄存器是指 256 位的寄存器。你的电脑里，每个核心可能有 32 个这样的寄存器。

希望目前为止大家都听懂了。

现在转到什么是子串搜索（substring search）。有问题吗？

**主持人**：是的，Ashot，聊天室里已经有两个问题了。第一个问题：在同样的数据处理任务中，Grep 的速度表现如何？你测试过吗？

**Ashot**：我想……说实话，我没看过 Grep 的源代码，但还有另一个工具，好像叫 `wc`（或者是说类似 `ripgrep` 之类的工具），用于计算单词或其他标记。我记得 Linux 中这个工具的优化是 SIMD 爱好者之间的一场长期争论，因为最初的解决方案当然没有向量化指令。Linux 是一个相当古老的平台，而向量化指令并非一直存在，尤其是像 256 位这样的指令。关于 Grep 我不能确切地说。你们可以自己试一试。还有问题吗？

**主持人**：是的，第二个问题。在你的计算中使用了多大尺寸的 needle（针/子串）？

**Ashot**：我们会谈到这个的，这个问题有点超前了。

好了，我们有子串搜索任务。为此，我们首先定义两个数组的相等比较操作。这里我们的数组类型是 `char`。如果你熟悉 STL，你可能喜欢 `std::string_view`。我也很喜欢它。它已经实现了逐对比较操作。

如果我们想手动实现它，第一种方法可能是这样的：检查相等性。如果数组大小相等，则逐个比较元素直到发现第一个不匹配。

```cpp
// 伪代码示例逻辑
if (size1 != size2) return false;
for (size_t i = 0; i < size1; ++i) {
    if (ptr1[i] != ptr2[i]) return false;
}
return true;
```

但这代码有问题，因为它有三个退出路径（exit path）：除了最上面的条件，还有 for 循环内部以及最底部的结尾。这是一个问题。其次，我们在字符串内部进行随机访问。所以我们做一个小技巧，去掉一层内部条件，将其移到 for 循环中，检查我们是否到达了循环末尾或者提前找到了匹配。

我解释这些是因为如果你现在打开基准测试并在旁边跟着看，可能会觉得代码有点复杂。刚才那段代码可以进一步加速和简化，指望编译器不会为其生成多余代码。方法如下：假设两个字符串最初就是相等的，我们在调用此函数之前已经处理好了这点。与其在字符串内部进行随机访问（random access），不如成对递增两个迭代器，检查具体地址的字符。

好的，这个操作我们有了。现在引入另一个抽象。我们所有的基准测试都基于一个需求：我们想遍历大字符串中子串的所有匹配项。也就是说，仅仅调用 `std::string::find` 得到一个答案是不够的。我们需要处理数百兆或几千兆的文本。

为此我们有一个 `findAll` 函数。它接收两个 span（可以理解为 `std::span` 的某种模板实例化）、一个引擎（engine）和一个回调（callback）。Callback 只是一个用于计数匹配和计时的函数。Engine 是我们要实现的东西。我们会创建几个 Engine。一个完全基于 STL 功能，另一个使用原始方法，接下来的 Engine 将使用 SIMD Intrinsics。我们的任务是比较几个 Engine 和几种编写子串搜索的方法，理解为什么一种比另一种快。

例如，我们可以编写一个朴素版本的 Engine，称为 `naive type`。输入 haystack 和 needle。然后在 haystack 上一次移动一个字符。并检查从该字符开始是否与 needle 匹配。这就是所谓的 brute force（暴力破解）。

显然这不是最快的解决方案，但让我们比较一下我们有什么玩具，我们在什么上进行测试。我尽量选取了几种不同的平台，以便我们有更具代表性的图景。

首先，有些搞数据科学的人，需要比较如果在抽象层面上深入一级，他们相比 STL 会损失或获得多少性能。STL 版本和我们的朴素实现的表现其实非常非常相似。而且这并不怎么取决于我在什么平台上：IoT（物联网设备）、笔记本电脑还是服务器。也就是说我们可以假设 `std::string` 库的实现者完全没有使用 SIMD 指令，也没有特别的优化方法。

现在让我们亲自认识一下参赛选手。我们有一个 IoT 开发板。就是这个玩具（展示 Jetson Nano）。这是 Jetson Nano 板，我用来做机器学习相关的测试。它的总功耗是 30 瓦，其中处理器占 4 瓦，它是 8 核的。还有我现在用来跟你们交流的笔记本电脑。还有一位同事友情提供的服务器。

这三种设备的功耗截然不同。如果说 IoT 设备的处理器消耗 4 瓦，那么服务器有两个处理器，根据型号不同，散热设计功耗（TDP）在 140 到 240 瓦左右。而且服务器里可能有两颗这样的处理器，如果是 Intel Xeon Platinum，甚至可能有 8 颗。

我们要看到的是，无论使用什么编译器和平台，标准方法和朴素方法之间是旗鼓相当的。

现在让我们想想……最大的问题之一是处理器里有不同大小的寄存器。与其只利用哪怕是大的标量寄存器（我们还没谈到 256 或 512 位的向量寄存器，即使是标量寄存器也有不同大小），只使用 8 位寄存器或者大寄存器的 8 位部分进行逐字比较，这对我们的效率来说是极其低下的。

这让我想起……Rabin-Karp 算法。如果我们有一个单词数组，如何找到需要的单词？我们可以计算每个单独单词的哈希函数，然后不再是逐字比较（每个单词可能有 10 或 20 个 8 位字符），而是都压缩成哈希来比较。这是一个好方法，但在纯粹形式下意味着我们需要预处理（preprocessing）。我们必须把文本分割成单词，然后计算所有单词的哈希，还要指望我们的 needle（针）永远不包含单词的一部分，也就是完全只是独立的单词。

那么我们做如下操作：与其对干草堆中的每个偏移量（offset）立即调用 `areEqual` 函数，我们先**比较输入子串和当前字符串片段的前四个字符**。如果前四个字符匹配，我们再检查剩下的。为了比较前四个字符是否匹配，我们创建了一个变量 `n-prefix`，甚至是常量。它的大小是 32 位，我们将字符串的前 4 个字节写入其中。我们先成对比较这 32 位与每个 offset，然后再比较剩余部分。

理论上，我们增加了一层 if 语句，一层条件，应该会变慢。但结果却是快得多。也就是说，算法的渐进复杂度没变，但我们获得了显著的性能提升。我们将前四个字符作为 `int32` 进行比较，然后再作为普通字符串比较的方法，带来了 **3-4 倍**的性能提升。这已经是……

**主持人**：Ashot，请稍等一下好吗？我冒昧打断你一下。带宽似乎不够了，视频和音频有点卡。能不能请你关掉摄像头，把更多带宽留给演示文稿？

**Ashot**：我想可以。让我试试，为了提高体验质量。好了，我关掉了。好点了吗？

**主持人**：好了，现在正在检查。这下大家就不能欣赏我的容颜了，好吧。我们到了下一个问答环节。我准备好听了。Sasha，有消息吗？

**主持人（Sasha）**：是的，还有问题。第一个问题：有人说标准库在字符比较函数中也使用了 SIMD，并扔了一个 LLVM 代码的链接。

**Ashot**：可能只是对于那个特定函数不是这样。也可能是方法不同。我见过有人试图用朴素的方式，也就是跟你比较 needle 所有字母和字符串所有 offset 一样的算法，仅仅是用 SIMD 指令来加速它。如果他们这样做，可能会有性能提升，但这并不显著。我们会分析另一种方法，它正是基于我在 `prefix type` 中的想法发展而来的。即先比较前四个字符，这是一种启发式方法（heuristic），但效果非常好。还有问题吗？

**主持人**：有。你试过将你的方法与通过正则表达式搜索进行比较吗？正则表达式内部实现了 Aho-Corasick 算法。也就是说，渐进复杂度上它应该更快。

**Ashot**：是的。正则表达式这完全是另一个话题，说实话。我准备好再开 10 个小时关于它的讲座。我们在最后回到这个话题。我的解决方案比通常与正则表达式相关的算法要快得多。世界上只有一个正则表达式库能接近这个速度，而且并非在所有情况下。而且那不是标准实现。

好，让我们试着理解这里发生了什么，以及我们接下来能做什么。

这里有真正的 SIMD 指令，理论上你们应该已经熟悉它们是什么，怎么构建的，或许甚至自己用过。我们做一个简单的操作。之前的实现版本中，在这个 for 循环里，我们从干草堆的每个单独字符开始，检查接下来的字符。

如果我们**一次跳跃 32 个字符**，并且在这个 32 字符的区间内，所有比较都进行向量化处理，会怎样？在这种情况下，我们需要做 4 次向量化比较操作，针对从第 1 个字符、第 2 个字符、第 3 个字符和第 4 个字符开始的 8 个 32 位前缀。因为我们总共比较 4 个字符的匹配度，我们需要测量这两个内存块所有可能的相互交叉点的相等性。

所以我们在 4 个 load 操作之后进行 4 个比较操作。这里我们使用的是 `load unaligned`，即内存地址未对齐。我们会回到这点，这很重要。我们执行这 8 个 32 位数与 8 个 32 位前缀的比较操作。

接下来我们有一个掩码，可以称为 `any`。检查干草堆的任何一个片段与针的前缀是否匹配。如果其中任何一个匹配，我们就可以将 256 位的掩码压缩成更小的掩码，这很容易检查，并且定义了转换为 `bool` 的操作符。所以我们使用 `moveMask` 函数（这也是一个 intrinsic），从 256 位中的每个 8 位部分取最高位。结果掩码进入一个 32 位的整数。

我们在这里还可以做什么？我们可以做得更复杂。我们可以通过已经获得的掩码 h0, h1, h2, h3 明确知道匹配在哪里，哪里需要检查。但如果我们试图单独解码每个掩码，这会花费太多时间，太多代码。在编程方面这不是很高效，我们也得不到巨大的性能收益。因为通常字符串中的匹配并不那么频繁。

所以，与其单独解码 H0, H1, H2 和 H3，我只是把它们混合成一团（OR 操作），检查这团里是否有任何一个正位（positive bit）。如果有，那么掩码被满足。在这 32 位中有某个匹配的开始。然后我们就用已知的算法单独检查这 32 位中的这一块。

现在看看我们用了哪些 Intrinsics。实际上，集合非常小。虽然你在 Intel 网站上看到的 Intrinsics 集合包含数百个 Intrinsics 和数千条指令，但大部分，正如我们在讲座快结束时会看到的，你并不需要。甚至可以说，在我看来，从生产力角度出发，有些是不受欢迎的。

首先我们使用 `Set1` Intrinsic。它取一个 256 位寄存器，用传入的常量的副本填充它。在我们的例子中，我们取针的前 32 位作为这个常量。
下一个操作是 `Load Unaligned`。我们显式地要求编译器加载内存地址。其实不一定要这样做，但这有助于我理解。
做了 `Load Unaligned` 之后，我们用 `Compare Equality` Intrinsic，我们在讲座开始时就见过了。`Compare Equality` 会输出大概像第二个绿色块里描述的那样。我们会有 8 个 32 位掩码，每个都在检查两个传入数组中成对的 8 个 32 位前缀是否相等。

也就是说，`Compare Equality` 是一个普通的比较操作符，接收两个参数，但因为是两个向量参数，它也有向量输出。又因为它方便处理同样大小的寄存器，我们不能只得到 8 个布尔值。它不返回 8 个 boolean，而是返回 8 个 int。

然后我们用普通的逻辑 OR 将这些匹配压缩成一个整体，并使用 `MoveMask` 从 `M256i` 提取出一个 `int`，再从 `int` 提取出 `bool` 放入条件体。

这给了我们什么？使用 **AVX-2 指令**，即使是以这种基础的方式加速我们的前缀方法，我们也已经瞬间将解决方案加速了 **3 倍**。速度从 3.3 GB/s 变成了 8.5 GB/s。在服务器上从 3.5 变成了 10.5。加速正好是三倍。

再次注意，在 IoT 这一栏我们没有数据，因为 IoT 用的是 ARM 处理器。ARM 处理器不支持 x86 指令。它有自己的向量指令集，Neon 和 SVE。我们也会谈到它们。

现在让我们谈谈很酷的东西：**推测（Speculations）**。不是金融投机，是计算推测。

让我们这样做：`OR` 操作放在计算 `MoveMask` **之后**，而不是之前。也就是上次我们计算了几个四个单独值的 OR，然后从四个参数的 OR 中提取掩码。这次我们试着反过来。先提取掩码，然后再做 OR。而且，把这些掩码分散到四个单独的变量中。

看起来我们的操作变多了。上面是原来的样子，下面是现在的样子。例如，原来只有一个 `MoveMask`，现在有四个。原来有三个 OR，现在还是三个。也就是说，本质上我们只是增加了三个 `MoveMask` 指令。

理论上，我们不应该获得性能提升。相反，应该变差。因为连助记符都是一样的。我们没有任何其他不同的新函数调用。

但实际上结果恰恰相反。

这个函数，或者说这种方法，这个 engine 我们称之为 `Speculative AVX2`，也就是我们刚刚看到的。它还有一个 AVX-512 的对应版本。区别在于一个使用 256 位寄存器，另一个用 512 位。同样我也写了 ARM Neon 的实现。

如我们所见，几乎在每种情况下，相比前一个版本，我们都获得了显著的性能提升。特别是在笔记本电脑上，我们要么从 8.5 GB 到了 12 GB/s。其实这里我有点说错了，在服务器上有时指标会有差异。也就是说有时 Speculative 可能会稍慢一点，但在大多数其他这里没特别列出的平台上，带有推测执行的版本运行得明显更快。同样在 ARM Neon 上也是如此。它比普通的 SIMD 前缀方法或者没有推测执行的前缀方法工作得更好。

这大约 **30% 的差异**，我不只在这个实验中观察到，在很多其他做 SIMD 指令的实验中也观察到。有些变体阻止了推测执行，有些则允许。而不知为何，一种比另一种快。

让我们理解推测执行的界限在哪里。在我们深入之前，可以回顾一个大概 3-4 年前的故事。它来自白帽社区（White Hat community），即那些发布结果而不是利用结果中饱私囊的黑客。那是 Intel 处理器中的两个漏洞：**Spectre 和 Meltdown**。

它们都与命令的推测执行有关，也就是一个线程从另一个进程的寄存器中窃取数据……一个程序、一个进程从另一个进程的寄存器中窃取数据。大概在那个时候，人们第一次开始思考什么是真正的推测执行，我们是否真的需要它。如果我们会失去安全性，数据可能出现漏洞。

结果发现，当 Intel 出来说“这是补丁，你们可以安装在服务器上，推测执行将被禁用”时，性能下降在某些情况下达到了 30-40%。这正是我们现在看到的数字。

自然而然的问题是，如何预先知道推测执行的级别是多少，以及理论上如何限制它。首先想到的就是去 intel.com，看看我笔记本里是什么处理器，看看上面写了什么。但遗憾的是，细节并不多。我觉得以前 Intel 的网站设计更差，但内容更好。现在关于 Cache 的细节，我唯一能读到的就是总共大约 16 MB。或者是 L3 16 MB。关于它如何在 L1、L2 和 L3 之间分配的细节，我不知道。

所以去 **WikiChip**。WikiChip 上有关于我处理器的页面。这里写着确实是 16 MB。这不是总缓存，是三级缓存。此外我们还有一级缓存。它几乎总是分为指令（Instructions）和数据（Data）。有时容量不是一比一对应的，但在这种情况下我们很幸运。理论上我们有 256 KB 的库，可以预加载接下来要执行的指令。

理论上你会想，那推测执行是不是允许向前查看这 256 KB 的后续指令？这不是真的，而且很难找到正确的信息。维基百科上有几个页面讲这个，但内容非常贫乏。而且，这全都是商业机密。像 Intel 和 AMD 这样的公司，不想彼此分享昂贵的测试和试验的成果。

所以我找到的最好的东西是大约 10-20 年前的学术论文，关于如何为处理器制作最优的乱序执行队列（out-of-order execution queue）。结果发现有各种权衡，当时的研究人员觉得 **16 条接下来的 load/store 指令**是比较理想的。实际上，如果你看我仓库里的其他例子（这次讲座我们不会覆盖），你会看到过度的循环展开（loop unrolling），或者太多的混合优化变体，并不会给你带来巨大的性能预判。你无法获得更多的预判，很可能是因为推测执行有一个虽然不高但仍然明显的限制，以及它能给我们带来的东西是有限的。

这就讲完了。我们慢慢过渡到下一个有趣的阶段，但我想现在展示一下 ARM 和 x86 的汇编代码有何不同是很重要的。这里我稍微道个歉，变量名有点不同，但你们可以相信，这段代码不仅仅是为了演示，我在生产环境中真的在使用。经常在不同特化的函数中，变量名会略有不同。但你们可以看到结构上的相似性。

首先我们将前缀与不同 offset 的子串进行比较。我们也有 `load` 操作，也有 `compareEquality`。只是在 ARM 的情况下它叫 `vectorEquality`，后面我们写 `u32` 而不是 `epi32`。此后我们也同样做 OR，提取掩码，提取 boolean，然后在条件体中使用这个 boolean。

如果专门谈到 **ARM Neon**，它与 AVX 指令非常非常相似，特别是在处理一维数据或一维向量、一维数组的基础常见操作时。你可以看到结果值的类型，不像 x86 那样是某种不明觉厉的 `m256i`，这里更具可读性，是 `u32` 乘以 4。顺便说一下，这里不是乘以 8，因为在 Neon 的原生实现中 ARM 没有 256 位寄存器。未来可能会有，但现在只有 128 位。所以只能放 4 个字符，总共 4 次比较。

关于 ARM 能说什么？ARM 是非常非常高效的平台。我拿了这个手上的嵌入式开发板，它有个很酷的功能。你可以不用重启设备就关闭几个核心，甚至改变整个系统的功耗水平。总的来说它的上限是处理器 4 瓦，处理器有 8 个核。IoT、笔记本和服务器里的处理器大概都是同一代的，大概是 19 年左右。但实际上 ARM 那时已经采用了低于 12 纳米的工艺，因为那时苹果手机好像已经是 10 纳米，甚至 8 纳米了。今年他们转到了 5 纳米。虽然这也是相对指标。一处的 14 纳米和另一处的 5 纳米可能是一回事。

这里表格最重要的是哪里？最重要的是底下一栏。**字节每焦耳（Bytes per Joule）**。这是我自己亲手测算的比较，不是从学术论文里拿的。这也是你们可以相信的原因。在 ARM 上使用 SIMD 指令所能达到的峰值性能是：每消耗 1 焦耳能量分析 4.2 GB 文本。

而我们在服务器上使用 AVX2、推测执行的 SIMD 代码，大概输出 1.6 GB 每焦耳。如果我们考虑到大多数人不写 AVX2 指令或 AVX512 指令，并且记得原始代码速度不是 10-12 GB/s，而是大概 3 GB/s，也就是慢 3 倍，那我们看到 ARM 是 4.2 GB/J，而目前世界上运行的大部分未针对 SIMD 优化的代码，最好的情况下能效大约是 0.5 GB/J，即 500 MB/J。即使根据我们当前的测量，能效也低了 **8 倍**。

现在这一张幻灯片是昨天才加进去的。这是新款 Apple 笔记本的初步基准测试，他们好像前天刚在发布会上展示，这是过去两个月里他们第三次展示向 ARM 处理器的过渡。中间那行是 MacBook Air 2020。这是一个轻量级小笔记本，13 英寸屏幕，被动散热，8 核处理器，功耗仅 10 瓦。

我现在用来跟你们说话的笔记本，可以说是这次比较中倒数第二的。MacBook Pro 16 英寸，2019 年底款。8 核，顶级 Intel，标称功耗 45 瓦，非常高的 Boost 频率，虽然由于过热问题它从未达到过那个频率。

但即使我们拿这些综合基准测试结果来看，我们也看到，甚至不需要拿优秀的 ARM 代码和普通的 AVX/x86 代码比，即便是优秀的 ARM 代码和优秀的 Intel 代码相比，今天的能效差异也是 **5 倍**。一台 10 瓦处理器、被动散热的笔记本已经超过了我这台 8 核 Intel 处理器的笔记本。

在我们进入下一部分之前，有一个额外的幻灯片，可以说这是关于什么是**自动向量化（Auto-vectorization）**的。可以说这是最后一刻加进去的。它是为了比较现代编译器有多聪明，看看即使在像子串搜索这样微不足道的任务中，它们能否帮我们优化。

自动向量化代码指的是那些 pragma 指令。也就是你通常放在 for 循环前的那一行，比如 `pragma for loop unroll` 之类，指望你的代码会加速。

我们得到了非常不同的结果。服务器上的自动向量化给出了可感知但在很小的性能提升。依然远低于我们要手写代码得到的。在 LLVM（即 Clang）的情况下，虽然不是最新版本，而是苹果支持的版本，我没有得到性能提升，甚至退化到了 1.5 GB/s。

所以你在选择使用哪种方法时有一个很强的权衡（trade-off）。如果你用汇编或 SIMD Intrinsics 写代码，你需要为每个平台单独写代码。为 x86，为 ARM 等等。但如果你使用自动向量化，你可能会想“啊，它现在能帮我解决所有问题”。但这也没那么简单，因为不同的编译器有不同的自动向量化指令。

所以一种情况下你有硬件灵活性（hardware flexibility），另一种情况下是软件灵活性（software flexibility）。一种情况下你要针对每个单独的编译器进行优化，还不一定能获得提升；另一种情况下你直接针对不同平台优化，这种情况下你很可能会获得非常非常明显的性能提升。

好，巨大的部分结束了，我准备好回答问题了。

**主持人**：是的，Ashot，有很多问题问你。第一个关于最早的 AVX 算法实现。

**Ashot**：Sasha，我能请你挑几个关键的吗？因为我们还有一些有趣的东西想讨论。后面会有一个长时段可以提问和讨论任何事情。所以请选现在最重要的。

**主持人**：好。第一个问题：为什么 needle 的大小是 4 个字符，而不是 2 或 8？为什么按 4 个字符比较更好？

**Ashot**：好问题。按 2 个字符比较非常不方便，因为处理 16 位内存子块的 AVX 指令非常少。有 8、32、64 是最常见的。此外，如果我们处理 ASCII 文本，这已经是启发式问题了。在两个字母中，如果我们只取前两个字母比较单词，单词之间的碰撞（collision）数量会相当大。也就是我们会经常得到假阳性（false positive），不得不进入完整的 for 循环，在里面分析每个单独的 offset。

假设 2 个字节可以编码 2 的 16 次方种变体。2 的 16 次方是多少？64000 种变体。但实际上 ASCII 字母并没有使用 8 位数字的全部频谱，文本中常关联的大概是 26x2 个字母加上 10 个标点符号。也就是说，即使在一个字节里也不是 2 的 8 次方，而大概是 50 种变体。这 50 种变体平方一下（因为取两个字母），得到 2500 种可能的组合。这太少了，所以经常会有假阳性。

比较超过 4 个首字母也不方便，因为我们想一次性比较尽可能多的不同单词和潜在匹配。如果我们取 64 位前缀，首先我们需要 needle 至少有 8 个字母，这比 4 个字母及以上的 needle 出现得少得多。其次，我们在一个处理器周期内能比较的 offset 数量会减半。明白了吗？

**主持人**：是的，很好的回答。好，太棒了。下一个问题关于推测性解决方案，为什么这里推测能起作用，代码里好像没有分支（branches）？

**Ashot**：推测不只涉及分支。推测是任何……这里我有错，抱歉。推测根据定义，至少是我赖以生存的定义，是指这样一种情况：程序员严格指定了指令顺序，而执行结果与预先声明或请求的不同。也就是说，假设处理器看到接下来的 5 条指令。第一条指令很长，在它看来会执行很久。它开始执行第一条，但在第一条结束之前，已经执行了第二、第三、第四条。这就叫推测执行。处理器可以自己重新组织代码执行顺序和相互依赖关系，以达到更高的性能。这就是推测执行。

在我们的例子中，回到上面那张图。在上面的版本中，我们先做 3 个 OR，再做 MoveMask。MoveMask 是相当复杂的操作。但在它开始执行之前，我们需要等待从主存加载多达 4 个内存块到寄存器，然后执行 3 次比较操作。也就是我们有非常严格的 memory bound，甚至严格的 data flow bound，严格的顺序，即一个操作严格依赖于之前的操作，不能在其之前执行。

因为 MoveMask 操作相当重且长，反直觉的是，做 4 个昂贵的 MoveMask 比做一个昂贵的 MoveMask 更有利。因为它们不是相互依赖的，我们为处理器的推测执行获得了更大的灵活性。就是这样。希望回答了。

**主持人**：是的。还有很多好问题，但我觉得回答起来要很久，我觉得可以把它们移到 Zoom 的讨论区。

**Ashot**：你说得对。取决于我加速到几倍音速，主会场可能还能剩点时间，我们可以再回来。

现在，Sasha，我们聊聊你的专业领域，工具（Tooling）和基准测试（Benchmarks）。这里我们将有一场可以说有点文学性质的“善恶之战”。我会试图吐槽 Intel，Sasha 可能会保护它。

关于 Intel 马上可以说的一点好话是，在所有硬件开发商中，他们的软件准备得最好。也许在操作和管理上比较复杂，但这可以说是唯一存在的东西。

如果你需要分析某段代码的性能，感谢 Sasha，我们要能运行 **Intel Advisor**。这是 Intel 开发的一个工具。据我理解，它的特点是进行更多的静态分析。但 Intel 还有第二个工具，**VTune**，它们互为补充。一个更多做运行时采样（sampling runtime），另一个更多是静态的。

当我们分析传来的代码时，我们看到了以下情况。屏幕中央你会看到两个大块。一个有漂亮的图表，显示 DRAM 带宽，并用大字写着“此循环主要是内存受限（Memory Bound），但也可能是计算受限（Compute Bound）”。这就是薛定谔的代码或循环。对于我们的编译器来说，它不理解那里到底发生了什么。但它能看到有大量的内存访问，并且经常出现 Cache Misses（缓存未命中）或 **Split Loads**。我们马上会回到这个。它觉得至少 52% 的性能限制与内存有关。

这种限制极难克服，因为它受限于物理。实际上几乎就是取决于你的主板上处理器和内存条之间焊了多少条内存通道。如果是新的服务器处理器，可能有 6-8 通道内存。在我们做测试的服务器上，很可能有 6 通道内存。我的笔记本是 4 通道。

但现在，如果我们打开 VTune，我们会看到一个非常有趣、重要的特性。它显示我们的 Memory Bound 实际上几乎全部（55% 中的 37%，即一大半）归咎于 **L1 split loads**。这是什么意思？

我们有处理器的 L1 缓存。因为我们要比较同一个文本缓冲区（这个干草堆）的每个可能的 offset（第 0 个字符、第 1 个、第 2 个、第 3 个），导致我们有大量的内存访问在地址上是**未对齐**的（unaligned）。但这是该任务的基本问题。

我们可以试图解决它：对于我们要比较 4 个字母前缀的情况，我们可以不做 1 个干草堆，而是做 4 个干草堆。抱歉，不是 1 个干草堆，是 4 个。第一个存整个字符串，第二个存从第 2 个字母开始的字符串，第三个存从第 3 个字母开始的，第四个存从第 4 个字母开始的。这种情况下，我们的 4 次比较操作请求就不是针对同一个内存块，而是针对 4 个单独的、按 32 字节边界对齐的内存块。

但这是一种糟糕的方法，因为虽然避免了 L1 bound 和 split loads，我们会遇到更糟糕的 memory bound，因为我们要对 4 个完全不同的内存块进行 4 次单独请求。与其把大量内存块加载给自己，不如让 split loads 即使数量巨大，哪怕有 cache misses，也要好过那样。

另一面令人悲伤的是，不知为何 L2 缓存未命中的数量依然比我预期的多得多。比如 L1 缓存有 190 亿次命中和 300 亿次未命中。我们试图搞清楚，但还没有找到客观、诚实的答案。

所以所有这些工具，它们有一定用处，但我认为它们有时使用起来很复杂，而且不是万能药。也就是说，应该只用它们分析最关键、最重要的部分。

你需要持续做的是，就像有围绕 Google Test 的测试驱动开发（TDD）一样，也有**基准测试驱动开发（Benchmark-driven development）**。这也是我比较喜欢的，也是围绕 Google 的库 **Google Benchmark**。它大概有这样的 API：注册基准测试名称，然后你有一个泛型函数，在我的例子中叫 `search`，它接收 engine。我把它传进去，指定最小运行时间。它可以运行更长时间。它自己决定运行多少次迭代。

然后它会在控制台给你这样一个表格，顺便给一个 JSON 文件，详细描述了在什么情况下你获得了什么性能。如果你运行我 GitHub 仓库里的代码，你会得到一模一样的表格。

我使用这些工具的纯个人结论是：Intel Advisor 和 Intel VTune 是互补组件，一个做更多静态分析，一个做更多运行时剖析。它们有一些只有经验丰富才能懂的特性，而我没有。Sasha 对此了解更多。比如，如果你做向量化代码，然后试图在 VTune 里看工具对你的代码向量化程度的评价。它会显示一个百分比或系数，但没写明的是，这个系数不考虑整数指令。它只考虑浮点运算指令。这些工具用起来不容易。

还有 **Godbolt**。Godbolt 给了我们一种直观的理解：指令少就是好。也就是通过 Godbolt 反汇编器生成代码，看看它，如果短，那就是快速、高质量的代码。但正如我们今天看到的，这不正确，因为操作的顺序，即使是相同的操作，也决定了很多东西。一种情况下指令更多，推测执行给你 30% 的提升；另一种情况下指令更少。理论上 Godbolt 会让你觉得那种方法更好，但实际上更差。

而在 Google Benchmark 中有一个陷阱：如果你在台式机上做完整的运行时基准测试，你必须关闭 **CPU 频率缩放（CPU Frequency Scaling）**。我们演讲的结尾正是关于这个。CPU 频率缩放是一种根据负载改变处理器频率的技术。比如你在玩游戏，你的角色从一边转向另一边，起初是一个简单的场景，渲染不难，然后你转向复杂的一边，为了不让用户体验下降，你的处理器会提升频率以防掉帧。在游戏中这很好，在基准测试中这是毁灭性的，因为仅仅是一个转向，你的性能结果就可能不同。

在笔记本电脑上不用担心，因为 Intel 放在我苹果笔记本里的处理器，标称功耗 45 瓦。基础频率 2.3 GHz，峰值 4.5。但我这辈子没见过我的笔记本跑到那个频率，即使我试图用某种人工方式关闭所有后台负载。也就是说在笔记本里，散热系统根本不足以让处理器加速。如果你在笔记本上做 Google Benchmark，令人惊讶的是，你的数据比台式机更具代表性。主要是要插上电源，别开启省电模式。

又一个高负荷环节结束了。来点问题。Sasha，请评论一下。

**主持人**：是的，这部分没有问题，但我还是想补充一下关于 VTune 和 Advisor 这类工具。其实工具的所有功能不只是看那些关于代码微架构效率的复杂图表。因为我们看这些结果时，一开始就选择了一个复杂的领域：看代码在微架构上的执行效率。在优化时，可能应该从简单点的开始。从热点（Hotspots）分析、线程（Threading）效率分析开始。然后逐渐推进到更难理解的部分。如果前几步已经取得了成功的结果，可能甚至不需要用到那些复杂的。

**Ashot**：合理的评论，同意。说得在理，完全正确。

现在让我们转到可能是最有趣的部分：**圣战（Holy Wars）、故事和 SIMD 的问题**。SIMD 市场项目。

写 SIMD 指令的程序员有他们内部永恒的争论话题。这个争论话题就是 **AVX-512**。这可能是整个 SIMD 指令世界中最具争议（controversial）的方面。

什么是 AVX-512？这是又一种向量指令，出现在 2015 年。正如我们从上面的表格看到的，这些向量指令大概每两年稳定出现一次。通常，由于 Intel 是 x86 处理器市场的主导力量，并不是 AMD 发明这些指令集，AMD 通常是跟进支持。但在这种情况下，并没有发生。

当 Intel 在 2015 年宣布 AVX-512 时，AMD 几乎立刻表示，我们根本不会支持这个指令集。这对市场和 SIMD 开发者的习得习惯是一个相当大的打击——出现了新指令集，就需要为它优化。因为现在最顶级的服务器处理器，虽然 Intel 员工可能不想承认，但其实是 AMD。AMD EPYC 处理器相比 Intel 处理器核心更多，频率更稳定，在合成和真实基准测试中结果更好。它们有 128 条 PCIe 4.0 通道，也就是说，比如说可以在一个处理器上连接十几块显卡，它们都会全速工作。可以连接数百个硬盘，几十个 NVMe SSD，这对我很重要。

AMD 拒绝了它。问题是，为什么？

第一个原因——**极其高的复杂度**。每年当出现新的助记符时，当出现新的操作名称如 compare, compare equal, add, multiply 等等时，你会惊讶，但这种助记符真的会周期性出现。看起来加法和乘法这种操作没那么多。但如果任务是不断压榨出性能增益，那就开始无中生有了。所以他们为不同类型的任务发明了一些新的、非常专业的助记符，以便该特定任务可能在新处理器上运行得更快。

也就是说，当出现比如 10 个新助记符时，并不意味着出现了 10 条新指令。正如我们了解的，指令和助记符是两码事。如果你将指令定义为助记符与类型甚至寄存器名称的组合，这意味着每一个新增的助记符都会为支持不仅是新的寄存器大小，还有所有旧的寄存器大小，带来数十条新指令。也就是说，如果 AVX-512 发明并添加了一个新的助记符用于某种狡猾的位掩码交叉，那么它不仅很可能需要为 512 位寄存器添加此指令，可能还要为 256 位寄存器做同样或类似的事情。这使得每一代新产品的指令数量剧增。现在助记符可能还没多到无法学习，特别是对 C++ 开发者来说，但指令的数量是巨大的。

另一个问题是，即使在 Intel 内部，**AVX-512 指令也极其碎片化**。AVX-512 有很多子集（subsets）。也就是下面那张表，每一列都是一个 AVX-512 子集。表格中的行是 AVX-512 发布后推出的处理器，有些部分支持，有些完全不支持。即使你使用某个 AVX-512 Intrinsic，你也不能保证这个 AVX-512 指令子集会存在于你使用的当前架构上。所以编译器可能会把它替换成另一个实际上更简单的指令。

这可能是最有趣、最神秘、文献中描述最少的部分。**许可证（Licenses）**。就像处理器有 L1、L2、L3 缓存一样（抱歉，不是 L0 缓存，L0 是许可证），处理器的许可证就是**处理器可以工作的频率**。

当你买下一个盒子，上面写着蓝色的 Intel 或红色的 AMD，通常上面有两个数字——基础频率（Base Frequency）和加速频率（Boost Frequency）。这是计算领域最大的谎言之一。根据执行什么指令以及在多少个核心上执行，别说 Turbo 频率了，即使是 Base 频率也会截然不同。

近年来，制造商在这个问题上变得稍微谨慎了一点，为了避开美国诉讼相关的问题。他们现在说，我们有一种 Turbo 在单核上工作，第二种 Turbo 在双核上工作，第三种 Turbo 在全核上工作。但除了频率取决于多少个核心在工作，还取决于你的代码是用什么指令写的，你的基础频率和加速频率都会有很大差异。

正如我们在这里从 WikiChip 网站的表格看到的，许可证被称为 Normal、AVX2 和 AVX512，但这并不是精确的命名法，更精确的命名法是 **L0、L1 和 L2**。在这种情况下，对于普通指令，简单的标量指令，处理器频率是 2.2 GHz，但如果你使用 AVX-512，即使是标称的基础频率也会降到 1.2 GHz。这对你的性能是 **45% 的打击**。

有非常广泛的术语集，只会增加关于频率问题的神秘感。Intel 的这些名字让我想起了 Apple 的营销计划，名字变了但本质没变。Enhanced Intel Speedstep Technology, Turbo Boost Technology, Speed Shift Technology, Turbo Boost Max Technology。就差 Super, Ultra, Mega 这些词了，但意思你们懂的。这些词背后隐藏着同一个总体概念：可以将所有指令或所有 Intrinsics 划分为某种层级。

简单指令和复杂指令的层级。说实话，这是一个非常复杂的话题。我试图用两种方式呈现它。第一种方式是表格形式。

如果我们看 L0 级别，这里有带 `MM` 前缀的指令，`MM128` 和另一个 `MM256`。第一条指令在 128 位向量寄存器上工作，第二条在 256 位上。它们实现了所谓的“简单操作”。也就是不仅是对整数的标量操作（如 `INT32`）属于这里，甚至向量指令，只要内容简单，比如整数加法，或者 AND-NOT（一种制作特定位掩码的方法，逻辑操作），甚至这些也属于 L0。这是廉价操作，你想用多少就用多少。不用担心处理器生你的气并降低频率。

L1 也包含一些 256 位操作。这部分你们必须记住的最重要的一点是：**整数乘法，特别是整数除法，是极其昂贵的操作**。因此，这种在向量寄存器上的操作会让整体操作变得更复杂。也就是说，32 位数的 256 位乘法是 L1 级别的操作，而这些数的加法是 L0 级别的。同样，简单的 512 位操作，如整数加法之类，也归入 L1。这里可能有点小错误，但不深入探讨了。

L2 级别是最复杂的操作。这已经是像其他 512 位操作之类的东西，例如**浮点变量（floating point）操作**。所有浮点变量操作都被认为是昂贵的。

现在再说一次，什么是简单和复杂操作。复杂操作例如：内存访问，所有浮点运算，整数乘法，以及 Shuffle/Blend 也是相当昂贵的操作。Shuffle/Blend 也是一种逻辑操作。简单操作是除此之外的所有操作。

当你先有简单操作，然后是复杂操作，然后是简单，然后又是复杂，或者它们混在一起时，处理器必须从一种模式（在一个频率下处理简单指令）调整到另一种模式（在另一个频率下处理更复杂的指令），然后再调回来。当它这样做时，有两种转换方式。称为 **Soft Transition（软转换）** 和 **Hard Transition（硬转换）**。

最危险的是 Hard Transition。硬转换只在一种情况下发生。如果你从 L0 转换到 AVX-512 指令。也就是说，如果你正在运行标量指令，然后在巨大的标量指令区里出现了一条 AVX-512，就可能发生硬转换。它的坏处在于它会导致 **CPU Halting（CPU 停顿）**。也就是你的处理器会思考一段时间，然后可能要过 25000 个处理器周期才能说出它是否成功转换到了新频率。必须完成到新频率的转换，然后才会执行你的指令。

我这里稍微加速一下，因为我们时间不多了，还有几件非常有趣的事情我想讲。

最后，我们的最终配方是什么？应该用什么？在哪种情况下，你的“香草”（普通）C++ 是不够的，需要转到 SIMD？

最大的性能提升发生在你做与**整数**相关的操作，或者例如**位掩码**操作，在 256 位寄存器中。这对于 x86 的 AVX2 来说已经足够大以获得性能提升，但又不够大导致频率下降。在 ARM Neon 上根本没有这样的指令，所以如果你在 ARM 上做整数操作，你有 128 位寄存器，但这已经足够获得良好的能效。

然后。对于 **Floating Point（浮点）** 操作。这正是自动向量化可能帮到你的地方。自动向量化在处理**数据并行（Data Parallel）**任务时表现得更好。也就是说，比如你有两个数组，每个有 1000 万个数，你需要成对相加这 1000 万个数，这是一个数据并行操作，内存访问模式非常非常简单。这样的操作编译器懂得优化。对于其他的，可以使用 128 位的浮点数操作，这样你就不会有频率问题。

还需要理解什么？**整数除法是一个巨大的问题**。在 Cannon Lake 上它要花费 50 个处理器周期。所以就像我们在普通标量代码中避免整数除法一样，在 SIMD 指令中也要避免它。幸运的是，这样的 SIMD 指令几乎没有，非常少。所以你犯错的空间并不大。

用什么我们知道了，现在的问题是，何时使用？

这里再次回到我去年的演讲：当你有大数据，并且需要快速处理时，像 GPU 这样的架构会来帮忙。但有些任务 GPU 帮不了你。

第一，如果你的数据批次（batch）小于显卡上的线程数。显卡上可能有 5000 个线程，而你的数组只有 500 个元素。为了每个人显然都知道是个愚蠢的主意：在一个 500 元素的数组上用 5000 个线程工作。

第二种情况。如果你的数据比显存（VRAM）大。VRAM 是显卡自带的内存。如果现代服务器可能有 2 甚至 10-12 TB 的内存（如果你很有钱），那么 VRAM，即显卡专属内存，通常只有 24 GB。而且这也是昂贵的显卡。所以，如果你的数据装不进，比如 24 GB，你很可能至少有一部分处理要在 CPU 上做，然后再在 GPU 上做。而在 CPU 上处理的那部分，应该用 SIMD 指令做。

或者还有一种情况，即使你有数据并行任务，但对你来说**延迟（Latency）比速度（Speed/Throughput）更重要**。如果延迟比吞吐量更重要，例如你有一个商业服务，客户来找你，你的任务是给客户一些推荐，展示他们想要的内容。如果你用显卡，通常的工作流是收集一千个这样的客户请求，为这一千人建立一个大矩阵（每行是一个客户的单独请求），然后把这个大矩阵扔给显卡，在那里可以说更高效地计算这个矩阵。但在你等待其他客户、构建大矩阵、将其传给显卡的过程中，你的客户已经走了。所以如果你做 **Deep Learning Inference（深度学习推理）**，即已经在使用机器学习而不是训练它，那么对你来说延迟比吞吐量更重要。

还有两个与我工作交叉的重要点。如果你的算法具有**亚线性（Sublinear）**执行复杂度，那么，例如从一个内存库复制数据到另一个是 O(N)。也就是如果你从处理器传到显卡，这是 O(N)。如果你的计算少于 O(N)，例如 log N，那么传给显卡也没有意义，因为传输的时间会比你渐进式的解决方案还长。

可能最有趣的应用是完全不是数据并行的任务，但你想加速它们。例如**压缩（Compression）**。这正是我在 Unum 项目内部数据库中使用的地方。当你需要压缩或解压通过网络或 SSD 飞速传来的数据时。或者你需要从一种格式编码到另一种格式，解析，通过网络传输以及所有相关的事情。

还有几分钟吗？

**主持人**：Ashot，遗憾的是没有几分钟了。只有一分钟，好的。我们甚至没有一分钟。我们只有 30 秒把你送进 Zoom 房间。遗憾的是连这点时间都没有了。

**Ashot**：好的，那我就用 20 秒说点东西。

还能实现什么？今天我们看了子串搜索。以下是我在不同封闭任务的库中大概实现的性能提升：

*   `std::regex` 的速度是 300 MB/s。我的 regex 速度是 14 GB/s。
*   Postgres 扫描数据的速度是 50 MB/s。我用自己的压缩和专门计算任务写的数据库，速度是 3 GB/s。
*   同样地，Elasticsearch 索引文本的速度是 10 MB/s，可以加速 45 倍。
*   即使是集合操作，如 `unordered_set` 查询，通常需要 150 ns，可以做到 15 ns。

一切掌握在你们手中，去学 SIMD 指令吧。

**主持人**：非常感谢你的演讲。我邀请你去 Zoom 房间回答问题，我们将对今天进行总结。再见。

**Ashot**：再见。谢谢。
