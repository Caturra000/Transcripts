# C++ 作为观察硬件的显微镜

标题：C++ as a Microscope Into Hardware

日期：2025/11/13

作者：Linus Boehm

链接：[https://www.youtube.com/watch?v=KFe6LCcDjL8](https://www.youtube.com/watch?v=KFe6LCcDjL8)

注意：此为 **AI 翻译生成** 的中文转录稿，详细说明请参阅仓库中的 [README](/README.md) 文件。

备注一：就是展示一些工具，另外 CPU 前端的讨论还是有价值的。

备注二：具体还是看原视频吧，现场有交互的。

-------

欢迎来到我的演讲：C++：洞察硬件的显微镜。今天，我们将尝试使用 C++ 来探究 CPU 是如何工作的。在开始之前，先简单介绍一下我自己。我叫 Linus，是一名 C++ 开发者，已经从业好几年了。我一直在自动驾驶和金融两个领域之间来回切换。目前，我在一家名为 EngineersGate 的公司工作，这是一家位于纽约的金融公司。我非常喜欢那里。他们重视员工，并且真正关心软件的设计和性能。所以，如果你想和我聊聊 EngineersGate，或者今天演讲中提到的任何内容，欢迎会后随时找我。

好了，我的名字是 Linus。所以法律要求我必须使用 Linux。因此，今天的演讲将聚焦于 x86-64 架构下的 Linux。但你不会... 不。

好的，我知道这里有很多人非常了解硬件和 CPU 架构，但让我们先假装我们一无所知。它就像一个能施展魔法的黑盒子。我们只知道两件事：第一，它有内存，因为当我们买电脑时，商家会告诉我们它有多少内存。第二，我们知道它能计算东西，因为它是一台计算机。在整个演讲中，我们希望不断完善我们对 CPU 的模型。希望到演讲结束时，我们的模型能够演进到足以很好地代表一个真实的、现代消费级 CPU 的样子。

我们实现这一目标的方式是，我们将提出关于 CPU 的问题，并尝试使用 C++、一些汇编语言以及我将要介绍的其他一些工具来回答这些问题。每当我们的模型与测试中观察到的现象出现差异时，我们就会完善和演进我们的模型，最终达到右侧那个更精确的模型。

我们为什么要这么做？嗯，我认为有两个原因。首先，这非常有趣。硬件工程师们为了让软件运行得尽可能快，所使用的技巧简直是疯狂的。其次，我认为，如果你关心性能，你就不能忽视你的程序所运行的硬件。你需要对理论性能极限有一个概念。而当理论极限与实际观察到的性能存在差异时，你需要能够找出问题所在。你需要了解硬件，也需要了解一些工具来调试你的程序。

所以，我要问的第一个问题是，我们到底是如何与 CPU 通信的？比如，我们如何告诉 CPU 要执行哪些操作？我们是软件工程师，所以很自然地，我们编写代码。这是一段极其简单的代码。然而，硬件并不理解 C++，所以我们首先必须把它编译成一个二进制文件。让我们来看看这个二进制文件长什么样。它是一堆乱码，因为 CPU 只理解二进制数据。不过，我们还是可以读到一些信息。这里的文件格式是 ELF 文件，这在 Linux 上很常用。里面有一些关于构建环境的信息。然后如果我们往下滚动一点，ELF 文件被分成了不同的段（sections）。今天我们主要关心的是 `.text` 段，可执行代码就存放在这里。

> 演讲者问观众：“你知道为什么它叫 text 吗？”  
> 观众：“不知道。”  
> 演讲者：“我也一直在找答案。不知道。”  
> 观众：“好吧，我甚至没找过答案，但如果你都不知道，那我就不尝试了。”

好了。这显然不是 CPU 看待目标文件的方式。CPU 看到的是二进制格式，像这样。这和刚才的是同一个目标文件，只是以二进制形式转储了出来。如果我们作为软件工程师想解读它，我们可能会写一个程序来打开这个文件，尝试解释文件里所有的 1 和 0。一旦我们解释完了，我们就可以调用一个函数来模拟硬件的行为。这大概是我们大多数人在软件层面会做的事情。

硬件基本上也做同样的事情。只是，它的步骤不叫“打开-解释-运行”，而是叫做“取指-解码-执行”（fetch-decode-execute）。这是我们整个演讲中会反复回到的基本流水线。

不过，我想从“解码”阶段开始，因为我真的很想知道，所有这些 1 和 0 到底意味着什么？它们编码了什么信息？我们可以通过在软件中进行解码阶段来做到这一点。我们可以把二进制文件反汇编成汇编指令，你基本上可以在二进制和汇编之间来回转换。所以，让我们反汇编这个文件，看看它是什么样子。

我们将要使用的、今天介绍的第一个工具是 `objdump`。在整个演讲中我们会多次使用它。它就是读取二进制文件并将其反汇编。这是它的输出。我们可以再次看到二进制文件、目标文件的名称、文件格式。这是 `.text` 段的反汇编，也就是可执行代码所在的地方。

第一列只是偏移量。第二列是我们上一张幻灯片中看到的二进制数据。然后我们看到，这些数据编码了不同的指令。例如，这里编码了两条指令：`mov` 和 `ret`。有些指令带有操作数。`mov` 接受两个操作数，目标和源。而 `ret` 则不接受任何操作数。

现在，如果你想知道都有哪些指令，以及架构支持哪些指令，我们可以对不同的输入文件重复这个过程。这正是我们接下来要做的。

让我们从刚才看的那个程序（或者说函数）开始。我们马上就能看到，我们有不同长度的指令。所以，不是所有的指令都占用相同长度的编码。这和 ARM 不同，ARM 的所有指令都有固定的长度。在 x86 上，它们的长度可以在 1 字节到 15 字节之间（*注：演讲者口误为50字节，实际限制是15字节*）。我们可以已经看到，解码阶段有很多工作要做，因为它不仅要解码指令，还必须弄清楚指令有多长。

所有的指令都在供应商的手册中有详细说明，你可以打开它，那是一本巨著，它会告诉你所有可用指令的编码。

接着我们看到，如果你想返回一个值，这里我们通过 `EAX` 寄存器来完成。这就像一个约定，所有东西都通过 `EAX` 寄存器返回。这里我们返回一个 32 位的值，如果你想返回一个 64 位的值，我们使用同一个寄存器，但可以指定我们想使用寄存器的哪个部分。所以 `EAX` 使用的是低 32 位，而 `RAX` 使用的是整个寄存器。

现在让我们用 `-O1` 优化来编译它，现在我们能看到不同类型的寄存器。例如，这里的 `EAX` 寄存器是被调用者（callee）改变的，然后调用者（caller）能够观察到这个变化。对于基址指针（base pointer），在我们在函数里改变它之前，我们先把它保存在栈上，也就是把它的值压入栈中（`push`），然后在从函数返回之前，我们再把它从栈中弹出（`pop`）回到寄存器里，这样调用者就不会知道我们用过那个寄存器了。

是的，`RAX` 这种就叫做易失性寄存器（volatile register），或者说是调用者拥有的寄存器（caller-owned）。

在这个例子中，我们可以看到，如果你想传递一个值给函数，我们也可以为此使用寄存器。约定的其中一部分就是，对于多个参数，寄存器的使用顺序是什么样的。返回寄存器、易失性或调用者拥有的寄存器，以及哪些用于函数参数，这些全都在调用约定（calling convention）中指定了。再次强调，这并不是硬件要求的，这只是我们必须遵守的一个约定。如果我编译一个程序，并使用了别人的库，我们最好就如何传递值达成一致，否则它根本无法工作。Windows 有不同的调用约定，但幸运的是，编译器会处理这些，所以我们并不真的需要了解它。

我们可以让函数体更复杂一些，来看看更多可用的指令。如果我们有大量的参数要传递给一个函数，我们最终会用完寄存器，到那时我们就会使用内存。所以，在二进制文件中，我们也可以直接引用内存地址。

现在，我们可以继续这个过程，但这有点乏味和漫长。但我真的很想知道哪些指令是可用的，哪些是常见的。所以，我们可以采取另一种方法，就是直接 `objdump` 一个已有的二进制文件，并从中获取一些统计数据。如果我们 `objdump` 它，然后看看哪些寄存器被使用，它们被使用的频率如何，我们对 GCC 进行分析会得到这张图。`RAX` 被使用得最多，这很合理，因为它是返回寄存器，所以每当我们想从一个函数返回什么东西时，我们都必须使用 `RAX`。

如果我们这样做，我们可以得到下面这个概览。这里有 16 个我们可以引用的通用目的整数寄存器。还有更多用于 SIMD 和浮点数的寄存器，但为了本次演讲的目的，我们只关注这些。正如我提到的，你可以引用一个寄存器的不同子部分。这些是保留寄存器，我们不能使用，是 CPU 用的。这里是调用约定，这是参数传递的顺序。所有这些都是易失性寄存器，所以被调用者可以改变它们。然后有几个是调用者拥有的寄存器。

我们可以对指令做同样的事情，转储它们被使用的频率。你可以看到 `mov` 是迄今为止最常用的指令，所以大多数程序基本上就是在搬运数据。你可以看到，这前 12 条指令占据了所有被使用指令的 77%。所以你甚至不需要知道很多指令就能大致读懂汇编。所有这些指令都非常直观，没什么复杂的。`xor` 就是异或，`cmp` 就是比较，`jmp` 就是跳转。所以没那么难。我觉得可能只有两个比较奇怪的是 `mov`，因为它其实是复制（copy），而不是移动（move）。然后 `lea` 指令非常常用，它只是一种花哨的加法。如果你把它和 `add` 比较，`add` 做的差不多是 `a += b`，而 `lea` 做的是 `x = y + z`。它对于计算数组索引也非常有用，你可以用一条指令完成，因为它的第二个操作数还允许移位。所以，是的，非常有用，这就是它为什么被用得这么频繁。

在这一点上，我的问题是，我们能用这 12 条指令编写 GCC 的 70%。那么我们还需要更多吗？我们能用这 12 条指令写出整个 GCC 吗？或者还有其他我们可能需要添加以使其图灵完备的指令吗？

所以我 Google 了一下。你需要多少条 x86 指令才能实现图灵完备？有谁猜猜吗？

> 观众：“三条。” “四条。”

很接近了。结果是你只需要一条。

> 观众：“噢！”

有一篇论文叫做《Move is Turing Complete》（Move 指令是图灵完备的）。它的开篇第一句是：“x86 指令集过度复杂且冗余地多余”（The x86 instruction set is overcomplicated and redundantly redundant），这是一种很棒的说法来形容某物是冗余的。但这听起来极其难以置信，有点像个标题党。第二句话是：“当它被缩减到只剩一条指令时，它仍然是图灵完备的，这条指令显然就是 Move 指令。”

理论上，我们可以打开那篇论文，尝试推理并理解它，但我认为那会非常枯燥，我们只想在这里做实验。幸运的是，GitHub 上有个人也读了那篇论文，然后想：“我打赌我能为这个实现一个编译器。”所以现在，为了验证 Move 是否真的是图灵完备的，我们只需要找一个随意的 C 程序，用 `mov-cc` 编译器编译它，运行它，然后把输出和 GCC 编译的同一个程序的输出做比较。

我去了维基百科，找了一个随机的程序，它做什么、怎么做其实不重要，只要是个程序就行。然后我用 `mov-cc` 编译器编译了它，我们可以 `objdump` 它来验证它是否真的只使用了 `mov` 指令。所以，这里面一路全是 `mov`。

这只是截断的一部分，完整的二进制文件大概有三四千条 `mov`。有谁猜猜用 GCC 编译的同一个程序的 `objdump` 有多少行？`mov-only` 的是三四千行。

> 观众：“可能 100？” “120？”

不错，大概 50 行，我想。是的，52 行。我运行了两个程序，它们给出了相同的输出。如果你仍然不相信 `mov` 真的图灵完备，在同一个 GitHub 上，他们有一个只用 `mov` 指令实现的《毁灭战士》（Doom）版本。所以，如果你想玩，去试试吧。

> 观众：“性能怎么样？”

好问题。所以，如果我们从硬件的角度来看，如果我们只需要解码 `mov` 指令，并且我们只需要执行 `mov` 指令，那么硬件可以说比需要支持数百条指令的硬件要简单得多。所以，这里面肯定有什么猫腻。为了探究这一点，我将介绍我们要使用的下一个工具。我们已经用过 `objdump` 了，下一个工具是 `perf`。

`perf` 让我们能够访问硬件计数器，我们可以观察到 CPU 中发生的许许多多的事件。要查看所有可观察事件的完整列表，你可以运行 `perf list`。其中一些需要 `sudo` 权限，所以如果你用 `sudo perf list` 会得到更多。然后我们可以用 `perf` 来分析任何程序。这里有一个小例子。我指定了我感兴趣的硬件计数器，然后对 `ls` 命令运行它，`perf` 就会给我们下面的输出。我们可以看到总耗时、CPU 迁移次数、执行的指令数等等。

所以，让我们用 `perf` 来运行我们的 `mov-only` 版本的程序和 GCC 版本的程序，看看我们得到了什么。你可以看到，`mov-only` 版本比 GCC 版本慢了 2500 倍，并且执行了 800 倍的指令。

> 观众：“我没玩过这个。这是在 GitHub 页面上的，但渲染《毁灭战士》的每一帧……”  
> 演讲者：“你等不到《毁灭战士》加载完，它需要七个小时。”

所以，很明显，从性能角度来看，这完全没有意义。我的问题是，为什么会有人费这么大劲去写一个整个编译器，而没有任何理智的人会用它？幸运的是，GitHub 上有一个 FAQ，只有一个问题和一个答案。问题是：“你为什么要做这个？”答案是：“我觉得这会很有趣。”我必须同意。

好了，让我们回顾一下我们目前所做的。我们看了 ELF 文件规范、指令编码、调用约定。到这里，我想我们已经知道得足够多了，可以扔掉我们花哨的机械键盘，直接手写二进制代码了，这很不错。我们用了两个工具，`objdump` 和 `perf`。我们看了不同的指令和寄存器，但我们还没真正看过内存。我们知道我们可以引用内存，但我们不太清楚不同的东西在内存中的什么位置，它们是如何布局的。所以让我们来看看这个。我们可以用 C++ 来对此有个大致的了解。

我们创建一个 `vector`，元素是 `string` 和指针的 `pair`。现在我们可以把不同的项放进这个 `vector` 里。我们可以放一些全局变量和函数进去，还有一些栈和堆的指针，一些命令行参数和环境变量。然后我们可以用 `ranges` 库，通过一个投影来对它进行排序。我们按照指针地址来排序，然后我们就可以把它打印出来，来了解内存的布局。

我们得到的结果是，在最高的内存地址，我们有环境变量，然后是命令行参数，然后是向下生长的栈，然后是向上生长的堆，然后是全局变量和函数。这里的全局变量的地址实际上是编码在二进制文件里的。所以如果我们转储我们的二进制文件，我们会在某个地方找到那个地址。

我的问题是，如果两个程序使用了相同的内存地址会怎样？比如，会发生什么？根据我们目前的模型，我们预期它们会互相干扰。这是我的第一个问题。第二个问题是，在栈和堆之间有一个巨大的地址空间，我很想知道中间发生了什么，比如我们有多少栈空间，多少堆空间。

让我们从第一个问题开始。我将同时在两个独立的进程中运行这个小程序。我们有一个 `atomic int`。它是原子的，因为我们将尝试从两个进程访问它。我们会增加这个全局变量，然后我们会打印出它的地址和值。我打印地址是为了真正展示两个进程使用的是完全相同的内存地址。根据我们目前所知，我们期望每个进程都在同一个内存位置上递增该值。所以总共会有两次递增，答案应该是 42。

如果我们运行它，我们实际上得到的结果是 41。这意味着我们在这里看到的内存地址不可能是真实的。它不可能是真实的物理内存地址。相反，它是一个虚拟内存地址。操作系统为每个进程维护一个映射表，一个从虚拟地址到物理内存地址的不同表格，这样不同的进程就不会互相干扰了。

我们早些时候看到 VCC 中有多少 `mov` 指令。所以很多指令都是内存访问。如果我们每次都必须请求操作系统来做虚拟地址到物理地址的转换，那将花费太长时间。所以，CPU 有一个缓存来缓存这个转换，这个缓存叫做转译后备缓冲器（Translation Look-aside Buffer，TLB），用来加速虚拟地址到物理地址的转换。

所以我们回答了我们的第一个问题。它们有不同的地址空间，不会互相干扰。

现在让我们来看看栈和堆。对于栈，我们有一个大小为 1MB 的结构体。我们可以在栈上实例化这个结构体的一个对象。然后我们会创建一个新的栈帧，并在其中创建另一个对象。我们会打印第一个对象和当前对象地址的差值。然后递归地调用这个 `measure_stack_size` 函数。最终，我们会用完栈空间然后崩溃。这就是发生的事情。你可以看到我们只有几兆字节的栈空间。

你可以对堆做类似的事情。这次我们用的是 1GB，而不是 1MB，大概堆空间会多一点。然后我们尝试在堆上一次又一次地分配这些 1GB 的空间，直到我们空间耗尽。如果我们这样做，我们发现我们可以分配 130TB 的内存，这对我来说没有意义，因为我保证我运行这个程序的机器没有 130TB 的 RAM。所以如果你在 `htop` 中查看，我们看到虚拟内存的数量确实非常接近打印出来的数值。但实际使用的内存几乎为零。所以当我们向操作系统请求内存时，它会很乐意给我们一个虚拟地址，但在我们真正需要它之前，它不会用物理地址来支持它。

那么下一个问题是，如果我们真的需要它会发生什么？如果我们真的访问了内存？这里有一个类似的程序。这次我不是无限地做，我只分配 30GB。我运行的机器有超过 30GB 的内存，我只是不想耗尽内存。那么，我需要在这里做什么最小的改动才能真正地访问内存呢？用字符数来衡量。

> 观众：“我需要对这个小片段做什么最小的改动才能让它真正访问内存？”  
> 演讲者：“是的，我能想到的最小的方式就是对所有东西进行值初始化。”

所以，两个字符的改动。现在让我们在 `perf` 下运行这个。左边的版本几乎不花时间，而右边的版本花了将近 20 秒。现在，原因可能是把 0 写入所有内存地址本身就需要那么长时间。这可能是一种解释。但为了绝对确定，我们可以运行... 我们可以检查所有我们能访问到的不同的 `perf` 统计数据，并在左右两个版本之间进行比较，看看是否能发现任何重大的差异。如果我们这样做，我们确实看到缺页中断（page faults）的数量在两个版本之间有很大的不同。

> 观众：“但这些是缺页中断，是次要缺页中断，只是为了我们... 并不是说你的系统内存耗尽了。”  
> 演讲者：“不，不，不是的。是次要缺页中断（minor page faults）。”  
> 观众：“好的，好的。”

所以次要缺页中断是这样的：我们有一个虚拟内存地址，现在我们真的想往里面写数据。所以操作系统就说：“哦，我给了他们一个虚拟内存地址，但它没有物理内存支持。所以我需要找一个物理内存地址，并建立映射。”这就是缺页中断发生时所做的事情。

> 观众：“我想补充一点对 C++ 非常重要的事情。所以当你写代码时，你想让一切都万无一失，你尝试，你分配 `new`，你 `catch bad_alloc`，什么也没发生，你继续写，然后程序崩溃了。这就是原因。”  
> 演讲者：“所以在现代 C++ 编程中，`malloc` 基本上总是会成功。”  
> 观众：“是的，是的，因为你有很多工作要做。”  
> 演讲者：“是的，是的，这就是为什么你需要在一个虚拟化环境中做这个。”

> 观众：“是的，这是 Linux 中的一种行为，他们称之为 ‘overcommit’，基本上他们总是懒惰地给你分配内存，但这实际上是可以禁用的。即使在 Linux 中，我忘了具体路径，但在系统里有一个配置项，如果你改变那里的一个数字，你就可以去掉这种行为，然后你就可以开始在 C++ 程序中看到内存分配失败了。”

> 观众：“好的。好的。酷。上张幻灯片有个快问。130TB 这个数字有什么特别之处吗？为什么它会在那里失败？”  
> 演讲者：“也许它碰到了栈内存的地址空间。我不太确定。硬件有 48 位的地址空间，也就是...是的。”  
> 观众：“但这是件好事。如果你运行的是 Linux，这是一个在 sysconfig 中可配置的东西。”  
> 演讲者：“好的。是的，所以有一个限制...他说 CPU 没有...也许在这种情况下，但也有一个高的操作系统...我的意思是...”

好的，让我们更深入地研究一下这些缺页中断。我们还不知道时间的增加是由于仅仅写入内存，还是因为缺页中断。所以让我们研究一下。为此，我们需要在我们的工具箱里再加一个东西：时间戳计数器（timestamp counter）。时间戳计数器是一个硬件计数器，在每个时钟周期都会加一。所以如果我们有一个结构体，在构造时读取它，在析构时再次读取它，我们就能计算出在这个作用域内花费了多少时间。这叫做 `ProfileScope`，这就是我们要用的。我把它扩展了一点。所以我们接下来要用的版本不仅记录了经过的时钟周期，还记录了发生的缺页中断。

这里我们分配了 3GB 的内存。我们测量它花了多长时间，然后我们会从这些字节中读取。所以我们会接触到这 3GB 范围内的每一个字节。我只是用了结果来确保它不会被优化掉之类的。所以下面我们会打印一个结果。我们会运行同一个循环两次。理论上，我们可能期望两次迭代花费完全相同的时间。但我们现在知道了，在第一次迭代中，我们可能会遇到缺页中断，因为我们还没有把它映射到物理地址空间。所以也许第一次运行会花更长的时间。让我们运行一下。

我们可以看到... 第一个块只是做 `mmap`。这几乎不花时间。这对大家来说够大吗？我把它放大一点。好了，好多了。

第一个块只是测量做 `mmap` 的时间，这几乎不花时间。第二个块是第一次迭代读取数据所需的时间。我们可以看到我们遇到了大量的缺页中断。运行时间占了总运行时间的 90%。在第二次迭代中，我们没有遇到任何缺页中断。我们仍然读取完全相同数量的内存，但我们快多了。

> 观众：“你是怎么计算有多少缺页中断的？是什么函数？”  
> 演讲者：“它是一个... 内建函数。是的，一个内建函数。”

好的，然后... 第二次当我们没有缺页中断时，我们快得多。所以我们可以看到缺页中断对性能有严重的影响。我们还看到... 我们仍然没有使用那么多物理内存。我们使用的虚拟内存是 3GB，符合预期，但我们使用的物理内存仍然微乎其微。原因在于，当我们只是读取时，我们不需要从不同的物理地址读取。我们只想读取零。所以操作系统将所有这些虚拟页映射到同一个零页（zero page）。然后我们会一遍又一遍地从同一个物理页读取。我们看到的另一件事是每次缺页中断对应的字节数。我们看到每次缺页中断是 4KB，这就是页大小。所以... 每当我们想请求一些内存时，它总是以页大小为单位。所以即使你只想请求一个字节，你仍然会得到 4KB。

现在让我们看看写入内存的情况。

同样，第一次读取比第二次读取慢得多。第一次写入也比第二次写入慢得多。因为在我们写入之前，所有这些虚拟内存地址都指向同一个只读的零页。当我们实际写入时，我们必须再次发生缺页中断。然后操作系统真的必须为我们找到不同的物理内存位置，以便我们能够写入。我们看到现在我们真的使用了物理内存，就像我预期的那样，2GB。

好的。是的，这就是内存的演示。

接下来，我很想知道我们能访问到的带宽是多少？我们读写内存的速度有多快？以及它可能如何依赖于我们想要访问的内存大小。对于第一个测试，我们将只进行顺序内存访问，并且我们会扫描我们访问的总范围。第二个测试，我们做同样的事情，但是以随机顺序。然后第三个测试，我们不扫描范围，我们总是迭代相同数量的内存。但相反，我们会改变步长（stride size）。所以第一次运行，我们会接触每一个字节。第二次运行，每隔一个字节。然后每隔两个字节，依此类推。

让我们从第一个测试开始。为此，我们再次扩展我们的工具箱。读取时间戳计数器非常方便。但如果你做微基准测试，它可能会波动很大。测量中可能会有很多噪音。所以有专门的基准测试库，它们会一遍又一遍地运行相同的测试，并对所有这些运行取平均值，直到它确定结果足够稳定，然后它会停止执行并打印结果。设置起来很简单。你只需要指定一个基准测试函数，然后把你的测试代码放在这个循环里。

所以让我们对内存写入做这个。我们会得到一些内存，总内存范围会从 1KB 扫描到 1GB，我们会以 2 的幂次增加。

然后我们就测量把一个值复制到每个内存位置需要多长时间。当我运行这个时，你会看到一列是时间。你可以忽略时间列，因为我没有根据我们迭代的总大小对其进行归一化。但还会有一列是数据速率，那一列是归一化了的。所以我们会看数据速率那一列。

在开始之前，让我们看看这生成的汇编。这是如果我们编译这个循环。我们看到，如预期的那样，我们把这个值复制到我们的目标地址。这是 `mov` 指令。这是我们唯一感兴趣并且想要测量的指令。在每次循环迭代中，我们还必须处理循环本身。所以这也将是我们测量的一部分，但我们并不真的希望它出现在我们的测量结果中，因为我们只想测量 `mov` 指令的性能。为了得到这个，我们可以把循环展开一点。所以我们就在每次循环迭代中做比一个 `mov` 更多的 `mov`。然后我们看到我们测量了更多的 `mov`，并减少了循环本身的影响。

这是在 `-O1` 优化下的。如果我用 `-O3` 编译，我们看到现在即使循环中只有一个赋值语句，我们在汇编里却得到了三个赋值。在实际的循环汇编里有两个赋值，然后在外面这里还有一个赋值。有谁知道这里发生了什么吗？是的，编译器也可以为我们展开循环。所以编译器把它展开了两次，然后它只需要处理循环迭代次数是奇数时的边界情况。所以我们可以展开循环，编译器可以展开循环，我们后面会发现甚至硬件也可以展开我们的循环。

好的，所以让我们把这个展开几次。这是展开了 32 次，然后我们可以运行它。是的，正如我早些时候提到的，你可以基本忽略时间列。更有趣的是这里的每秒字节数，因为那是根据整个范围归一化的。我们可以看到它在一段时间内保持稳定。然后带宽下降，但又在一段时间内保持稳定。这里有一堆 16 的。然后它又会变慢。让我们把这个画成图。

这就是那张图。如你所见，它开始时是 23 GB/s，然后下降到 16 GB/s。这和我们在这里看到的值是一样的。这里出现这些阶梯可能有点出乎意料。为了弄清楚这里发生了什么，我们可以选择三个位置。这里在 16KB，128KB，以及在慢速范围内的某个值。我们再次可以使用 `perf stat`，看看在这些不同区域之间是否有任何计数器出现偏差。

在 16KB 时，我们看到必须有一个缓存。在 16KB 时，我们没有得到那么多的缓存未命中。在 128KB 时，我们得到了很多 L1 缓存未命中。所以我们可能超出了 L1 缓存的范围。然后在这里，我们超出了所有缓存的范围。所以有三个不同的缓存级别。我们可以用 `lscpu` 来查看缓存级别。32KB 的 L1，256KB 的 L2。这和我们在这里测量的结果大致相符。这是一个比较粗略的测量，但至少它肯定是一致的。

好的。这是顺序写入。我接下来想做的测量是随机写入。这是随机写入的结果。在非常小的范围内，我们几乎是一样的，但随后性能就急剧下降。原因在于，当硬件意识到我们正在顺序访问不同的内存位置时，比如我们请求了字节 1、2、3、4，硬件意识到我们接下来可能请求字节 6、7、8、9、10。所以它会已经尝试把那些数据取到缓存中，这样当我们真的需要它们时，它们就已经准备好了。当我们随机地在内存中迭代时，硬件无法做到这一点，所以性能会大幅下降。这就是为什么 `vector` 这么棒。如果我们只是按顺序遍历内存，性能会比我们到处跳跃要好得多。

好的。我们做了顺序写入，随机写入。让我们谈谈步长访问（strided access）。我想，根据我们目前的模型，我们预期的结果会是这样。当我们访问每个字节时，我们很慢，然后当我们跳过更多字节时，我们会越来越快，因为我们需要加载到内存的数据变少了。所以我想这大概是我们会预期的。

当我们测量时，我们得到了一张像这样的图。这里发生了两件意想不到的事情。第一件是，我们在这个范围内没有变得更快。所以我们想知道这里发生了什么。然后第二件奇怪的事情是在 128 和 256 步长时出现的这些尖峰。

让我们逐一回答这两个问题。先从第一个开始。为什么我们在这里没有变得更快？这个下降点正好在 64 字节处。但让我们先假装它在 3 字节处。只是为了方便讨论，假设它在 3 字节处。然后我用这条小竖线标记了每组 3 个字节。所以如果我们访问每个字节，我们大概在这里。如果我们每隔一个字节访问，我们大概也在这里，但没有变快。如果我们每隔两个字节访问，我们正好在这个下降点，但仍然没有变快。然后最后，如果这个字节是 4 字节，我们终于进入了这个区域，我们变快了。然后如果我们跳过更多字节，我们会变得更快。

这背后的原因是，只有当我们开始跳过这些 3 字节的整个集合时，我们才会变快。原因是存在缓存行（cache lines）。所以在这个例子里，一个缓存行就是 3 字节。每当我们访问一个缓存行中的一个字节时，我们都必须把整个缓存行拉入缓存。所以即使我们实际上只使用了这里三分之一的字节，我们拉入缓存的数据量和上面这个例子是一样的。然后只有当我们开始跳过整个缓存行时，我们才真正变快。实际上，缓存行大小不是 3 字节，而是 64 字节。

好了。这解释了我们在这里看到的第一个现象。让我们谈谈这些尖峰。我想之前有人提到过，对于内存查找，我们只用了 48 位。但 CPU 的一项工作是，它必须判断我们请求的一个内存地址是否在缓存中。它必须能够极其快速且确定性地在几个周期内做到这一点。所以它忽略了最高的 16 位，并且它可以忽略最低的 6 位，因为我们知道那是在缓存行内部。如果你知道缓存行在缓存中，那么该缓存行中的每个字节也都会在缓存中。但是，一个缓存可以容纳大量的缓存行。所以理论上，我们必须把这些内存地址和当前在缓存中的每一个缓存行进行比较，这是非常昂贵的。

所以，缓存被分成了不同的组（sets）。最低的几位，这里的最低几位，被用来索引到一个特定的缓存组。所以如果我们假设这是我们的 L1 缓存，每一个条目都有一个 64 字节的缓存行。然后我们有一定数量的缓存组。所以总缓存大小是缓存行大小乘以路数（ways）再乘以缓存组数。然后因为我们可以索引到某个特定的缓存组，我们只需要检查这个缓存组中存在的几路，来确定这个地址是否在缓存中。

现在，如果步长是 128，那么这个内存地址的最低 7 位将永远保持不变。所以对于 128 的步长，这些位将永远保持不变。这意味着我们只使用了一半可用的缓存组。我们会跳过每隔一个可用的缓存组，这意味着我们基本上把可用的缓存减半了。这就是为什么我们看到那个尖峰，因为对于这个步长，我们只使用了总物理可用缓存的一半。对于 256 的步长，最低的 8 位会保持不变。所以在我们的例子中，我们会把可用缓存减少到四分之一。这就是为什么我们看到第二个尖峰。

> 观众：“那这个尖峰不是应该更大吗？”  
> 演讲者：“嗯，我们读的相对来说可能...但我们可能读的数据很少，以至于我们仍然能放进更低级的缓存。我没仔细研究过。但，是的，这是我测量的结果。我只能假设，我们读的太少了，以至于可能放进了更低级的缓存，所以影响没有那么大。”

好的，有什么问题吗？好的，我们继续。

好的，回顾一下内存。我们谈了内存布局，关于堆和栈。我们谈了虚拟内存，关于分页、TLB。我们谈了缓存，以及不同的缓存级别 L1、L2、L3。我们谈了缓存行、预取，我忘了提，你有的路数（number of ways）被称为缓存的关联度（associativity）。所以如果有人告诉你他们有一个 8 路组关联缓存，那只是说这个表里有多少行的一种花哨说法。好的，我们谈了预取、缓存关联度，以及我们用的一些工具：`htop`、时间戳计数器和 Google Benchmark。

演讲的最后一部分将聚焦于...我们已经谈了二进制文件本身，我们谈了内存。还缺少的部分是 CPU 中实际执行我们代码的部分。所以我们最后来谈谈这个。

为此，我们要在我们的工具箱里再加一个工具，`nasm`，它是一个汇编器。`nasm` 允许我们从汇编到二进制。我们之前用 `objdump` 从二进制到汇编，我们可以用 `nasm` 来反向操作。使用 `nasm` 的好处是我们能非常精确地控制 CPU 上将要执行什么。如果我们用 C++，我们必须经过编译器。编译器在弄清楚我们到底想做什么方面非常聪明。它们经常会优化掉很多代码，然后就很难让 CPU 去做一件非常具体的事情。而如果你直接控制汇编，这就容易多了。

要使用 `nasm`，我们得告诉 CMake 关于它的信息。这不需要太多的 CMake 代码。然后我们得自己写汇编程序。我们得把它放在 `.text` 段里，我们之前学过这个。然后我们就可以写汇编了。我们知道 `mov` 和 `add` 指令。我们也知道调用约定和所有东西。所以我们知道传递给这个函数的第一个参数会在 `RDI` 中。第二个参数会在 `RSI` 中。这个加法就像 `a += b`。然后我们加到 `RAX` 里，因为那是返回寄存器。从 C++ 中，我们可以把它声明为 `extern "C"` 来处理好名称修饰（name mangling）。然后我们就可以直接从 C++ 调用它。所以如果我们调用 `add(10, 20)`，我们会打印出结果 30。

让我们来谈谈 CPU 执行代码的地方。我们几次提到了“取指-解码-执行”流水线。通常，“取指”和“解码”被称为 CPU 的前端（front end）。然后“执行”阶段被称为 CPU 的后端（back end）。我接下来会用这些术语。所以请记住。前端是取指和解码。后端是执行阶段。

我这里有几个汇编片段。它们是成对出现的。这是第一对，第二对，第三对。每对之间的区别在于，第一个版本只是把这行汇编重复了 10 万次。这个 `%rep` 不是汇编的东西，这是 `nasm` 的东西。它的作用就是把 `rep` 和 `endrep` 之间的东西复制粘贴指定的次数。而这里我们是在一个循环里做同样的操作。所以我们不是在二进制文件里有 10 万行 `add rax, 9`，我们只是让它循环，但我们循环的次数是通过 `RDI` 传递给函数的。所以如果我们传入 10 万，那它就会循环 10 万次。

对于第一对，我们只是把一个数加到 `rax` 里。对于第二对，我们做一个 `nop`（无操作）。一个 `nop` 仍然需要被前端取指和解码。它会被分发到后端，但后端只会把它扔掉，什么也不做。所以第二对这里只是做 `nop`。然后第三对也做 `nop`，但它用了 `nop` 的不同编码。我们在开始时谈到过，供应商指定了每条指令的编码。`nop` 有不同的编码。你可以用一个字节来编码一个 `nop`，也可以用多个字节。所以这里我用了一个占用 9 个字节而不是 1 个字节的 `nop`。所以程序仍然不会执行任何东西，但二进制文件会更大。

所以让我们再做一个小演示。让我们从 `add` 开始。这是我幻灯片上的第一个代码示例，它只是做了 10 万次加法。所以再次，这个 `rep, endrep` 和我把这行 `add` 写 10 万次是一样的。看看这要运行多久。这运行了 32 微秒。现在我们知道在这 32 纳秒（*应为微秒*）内完成了 10 万次加法。所以我们可以计算出每次 `add` 指令需要多少个时钟周期。为了做到这一点，我们只需要知道时间、完成的加法次数，以及 CPU 的频率。如果你查一下这台 CPU 的频率，是 2.6 GHz。现在如果我们用 2.6 GHz，我们会计算出一条 `add` 指令耗时少于一个周期，这没有意义。

所以 CPU 可以在不同的电源状态下运行，它可以在较低的频率下运行以节省电力，也可以在性能关键时提高频率。`lscpu` 给出的只是默认频率。要获得实际的当前频率，我们可以用 `cpupower`。我们看到 CPU 当前实际上运行在 3.1 GHz。所以现在我们知道了所有需要的值来实际计算每次 `add` 的周期数。只需要回到这里的时间。所以让我们复制这个，然后把它粘贴到这里。然后如果我们执行它，我们得到每次 `add` 的周期数。一次 `add` 花费一个周期。

好的，然后如果我们把它放在一个循环里运行，现在我们不仅要执行 10 万条指令，我们还有这个 `dec`（递减）和这个 `jnz`（不为零则跳转）。所以现在我们不是 10 万条指令，我们总共将执行 30 万条指令。所以我们不太清楚会发生什么，但一个合理的假设可能是它会花费三倍的时间。但让我们运行一下，看看结果。

我们看到，尽管执行了三倍的指令，它花费的时间几乎完全一样。这告诉我们，前端，也就是取指和解码阶段，可以在每个时钟周期并行地解码多条指令。所以它需要取足够的数据，才能在一个时钟周期内解码多条指令。它还必须并行地解码它们。它还必须弄清楚下一条指令从哪里开始，因为我们有可变长度的指令。所以如果它想在一个周期内解码多条指令，它某种程度上必须猜测第二条指令从哪里开始。是的，而且显然它能在一个周期内完成所有这些。

所以如果我们想知道理论上我们一个周期能解码多少条指令，我们必须确保我们不受后端的限制。所以下一个例子，指令将是 `nop`。所以我们知道后端不需要做任何事。这将完全受前端限制。所以让我们执行这个。我们可以看到 `nop`，仍然是 10 万个 `nop`，但总执行时间是 `add` 版本的四分之一。所以我们知道我们每个周期最多可以解码四条指令。

现在让我们在循环里做同样的 `nop`。

> 观众：“当你做 `rep` 时，你运行了 10 万个 `nop`，但你只运行了一次还是测试运行了好几次？”  
> 演讲者：“测试，Google Benchmark 测试运行了 8000 次。这是这里的迭代次数。但我们看到的时间是总时间除以 8000。所以这是每次迭代的时间。”  
> 观众：“好的。那才合理。”  
> 演讲者：“是的，是的。”

好的。所以我们知道如果我们不受后端限制，我们每个周期可以解码四条指令。所以让我们运行这个 `nop` 的循环版本。好的，好的，好的。我们又回到了 32 微秒。原因在于这个 `dec rdi`。这和我们在这里做的 `add eax` 几乎是一样的事情。这是一个巨大的依赖链，因为你无法做下一次迭代的递减，直到你完成了上一次迭代的递减，因为你需要知道上一次迭代的值才能进行递减。

好的，好的。让我们切回幻灯片，看看关于这个的一些统计数据。我们已经做了 `add`。这里有更多的性能计数器。我们可以看到实际执行的指令数。前端的工作是始终为后端准备好许多待执行的指令。所以后端总是有足够的工作来尽可能多地并行执行指令。所以前端试图领先于后端，尽可能多地解码，然后把指令塞给后端，然后后端就可以挑选它一次可以运行哪些指令。这就是为什么我们在前端和后端之间有一个队列。所以前端只是尽可能多地把操作放进这个队列里。

如果你看这个 `add` 指令，它花了 32 微秒。我们分发...我们想执行 10 万条指令。这里的单位是千。我们向后端分发了 10 万条指令，然后后端实际执行了 10 万条指令。如果我们对循环做这个，我们看到了第一个奇怪的行为。如预期的那样，我们想执行的指令数是 30 万。但前端只向后端分发了 20 万条指令。原因在于，它分发给后端的指令和我们在汇编中看到的指令是不同的。原因在于，我们有一些汇编指令，比如 `add ex, 1`，后端需要做的和我们只是 `inc ex`（递增 ex）是完全一样的事情。所以汇编中可以有多种指令，它们解码成后端必须做的完全相同的指令或操作。所以前端就像一个从指令集中所有可用指令到后端实际可执行的所有指令的映射。后端实际有硬件支持的指令数量比我们在汇编中能访问的指令数量要少。所以这是一个更精简的指令集，这些微操作（micro-operations）。

所以后端执行的指令被称为微操作。我们并不真正知道它们是什么，因为供应商并不真的告诉我们。但是，是的，它们存在。

> 观众：“这个更低的数字不是暗示了微代码中有一个专门用于加一的指令吗？”  
> 演讲者：“是的。所以有些指令，比如 `add` 指令，映射是一对一的。所以一条 `add` 指令对应一个微操作。有些指令，一条汇编指令映射到多个微操作。所以可以是一对多。然后有些指令，比如这个 `dec jmp`，它们太常见了，以至于它们实际上映射到一个微操作。这个过程被称为微操作融合（micro-op fusion）。所以一些汇编指令可以被融合成一个微操作。”  
> 观众：“公平地说，前端也有一个循环...”  
> 观众：“微操作缓存。我们想到一块去了。”  
> 演讲者：“你是指？”  
> 观众：“不，不。这是从 8086 时代来的东西。有一个 `loop` 操作码，它在单个操作码里做这两件事。现在不用了，只是为了兼容旧版。”  
> 演讲者：“好的。所以你是说有一个 `loop` 汇编指令，它基本上把这两个融合成了一条指令。”  
> 观众：“是的，但我想它用的是 `RCX`。”  
> 演讲者：“好的。所以。好的。所以我们在 x86 上有很多指令，这就是为什么早先那篇论文说它‘过度复杂且冗余’。好的。”

对于 `nop`，我们变得更快了。我们分发 10 万条，我们可能执行 10 万条指令。我们分发 10 万条指令，但后端把它们全都扔掉了。所以这基本是预料之中的。在 `nop` 循环中没什么有趣的。

现在是 9 字节的 `nop`，后端仍然什么也不做。我们分发的指令数和这个 `nop` 是一样的，但我们明显慢了很多。那么原因是什么？我们可以再次查看所有的性能计数器，看看是否有什么有趣的，我们看到了这些指令缓存（iCache）未命中。我们早先在做带宽测量时看到了数据缓存。所以我们知道有数据缓存。也有指令缓存。指令和指令流以及数据流使用相同的 L3 和 L2 缓存，但 L1 缓存它们俩是不同的。原因只是出于延迟的考虑。所以你想让最低级的缓存尽可能靠近需要值的地方，只是为了让电子不必跑太远。所以 L1 指令缓存物理上就位于前端旁边。而 L1 数据缓存物理上非常靠近后端。

> 观众：“在 Intel 上这还不是全部。现代的 64 位 x86 CPU，当你启动它时，它处于实模式 86 模式。所以它基本上在模拟一个 70 年代末上市的 CPU。从那时起，CPU 架构发生了相当大的变化。所以 CPU 能做的操作码和做这些事的架构之间有非常非常强的脱节。Intel 做的是，当它第一次过机器语言时，它把指令解码到指令缓存中。所以它实际上是把旧的 CPU、旧的操作码翻译成它内部执行的更现代的操作码。我明白了。所以在 RISC 和 ARM 上，是的，这就是有独立指令缓存的原因。在 Intel 上还有另一个... 所以不仅是为了延迟，显然还有更现代的...然后它还计算哪些指令可以一起调度。”  
> 演讲者：“好的。有意思。好的。”

但是这些指令缓存未命中是我们慢这么多的原因。这里有两个缓存在起作用。一个是指令缓存，它只缓存最近使用的指令。还有一个是微操作缓存，它缓存最近的解码结果。所以如果我们看到一个我们之前解码过的指令指针，我们就不必再费力去解码它了。我们可以直接在微操作缓存中查找指令指针，并直接查找结果微操作。所以这里有两个缓存在起作用。然后如果我们循环同一条指令，而不是一条接一条的不同指令，我们就可以充分利用指令缓存和微操作缓存，我们又变快了。

> 观众：“我猜 `add` 的指令缓存未命中数 55 是个笔误。否则它会慢得多，我想。”  
> 演讲者：“我不认为是笔误。因为这里我们确实有一个大的二进制文件，因为我们有 10 万个 `add` 一个接一个。而这个 `add` 指令比这个 `nop` 需要更多字节来编码。所以有理由认为我们在这里比在这里有更多的指令缓存未命中。”  
> 观众：“等等。对，但和 `nop` 相比。不，这个 `add` 编码需要的字节比这个 9 字节的 `nop` 要少。所以这个的二进制大小会比这个的二进制大小小得多。”  
> 观众：“我想它没有出现在 `perf` 数字里是因为那个情况是受后端限制的。所以指令缓存是受后端限制的。”  
> 演讲者：“好的。明白了。谢谢。”

哦，是的。好的。关于这个还有更多问题吗？好的。

我一直在告诉你们，前端试图领先于后端，尽可能多地解码，以保持这个队列总是满的，这样后端才能执行东西。但这忽略了一个事实，就是我们这里有一个跳转。所以直到后端执行了这个跳转，我们并不知道我们是否走这个分支。那么前端怎么知道接下来要解码什么呢？为了找出...它不知道它应该解码这个 `ret` 之后的指令，还是应该再次解码这个 `add` 指令，直到后端实际执行了它。为了找出发生了什么，我们将使用另一个工具，叫做 `llvm-mca`，这是一个 LLVM 工具。它代表机器码分析器（Machine Code Analyzer）。它的作用是逐个周期地模拟一个 CPU 以及它执行的内容。

所以我们可以运行这个。我们可以指定我运行的机器的架构是一个 Haswell 芯片。这只是一个输出参数，为了得到我想展示的某种输出。然后我们指定汇编文件。如果我们运行这个，我们得到一些输出。这将模拟这四条指令。`add`、整数乘法、递减，然后是跳转。

如果我们向上滚动一点。所以这个模拟的是，每一列是一个周期。所以每一列是一个周期。每一行是一条指令。每个四行的块，因为我们的循环里有四条指令，对应一次循环迭代。现在我们可以看到 D 代表分派（dispatch）。所以即使这些指令对我们来说似乎应该是一个接一个执行的，它们都在第一个周期就被分派了。然后第一行的 `add` 指令立刻开始执行。我们早先测量过，一个 `add` 指令需要一个周期。所以它执行完毕，然后它退役（retires）。退役意味着它被提交到内存，并且对程序可见。

所以乘法指令在第一个周期被分派。但它必须等待 `add` 指令的结果。因为它对 `RCX` 进行平方，而 `RCX` 保存着 `add` 指令的结果。所以它必须等待一个周期，然后它开始执行。乘法需要三个周期来执行。然后它会准备好，一旦准备好它就会准备。

递减指令。

> 观众：“四个周期。是的，我想这些小的 'e' 代表它实际执行的时候。所以 `add` 只需要一个周期。但这不 100% 准确。所以请持保留态度。但它大多数时候是准确的。但比如内存获取，它无法知道会花多少周期。所以它只会假设一个值。”

好的，我们做完了乘法。然后是递减，它对 `mov` 或 `add` 没有任何数据依赖。所以它可以立刻执行。它在乘法开始的同一个周期内就执行完了。但它不能退役，直到上一条指令完成。然后是 `je`（相等则跳转），它必须等待递减的结果。一旦那个结果准备好了，它就会执行，然后在上一条指令退役后退役。

所以这都是第一次迭代。但我们看到，在第一次迭代的 `add` 执行完之前，我们已经分派了循环的第二次迭代。所以循环第二次迭代的递减。这里它甚至在循环第一次迭代的 `add` 完成时就完成了。而在循环第一次迭代的乘法甚至还没完成之前。所以它会同时执行循环的多次迭代，但它不会退役它们。它会同时执行它们，但它只会按正确的顺序退役它们。

所以这就是前端如何领先于后端的。它只是选择一个分支，就像它只是尝试猜测哪个分支会被采用。然后它解码那个分支。它把它分发给后端。后端会开始执行它。希望它猜对了。如果它猜对了，一切都好。我们可以在正确的时间退役它们。如果它猜错了，我们不会退役它们。所以我们做了一些不必要的工作。我们必须清空流水线，扔掉所有东西。我们做了一些我们不该做的工作。但它永远不会对程序可见，因为它永远不会被退役。

这对循环非常有效。因为对于循环，猜测很容易。你总是会跳回去，除了最后一次循环迭代。所以非常容易预测。它带来了巨大的性能提升。对于 `if-else` 分支，可能会更难一些。有些可能很容易预测。但有些可能非常难预测。

根据我们目前的理解，我们预期一个容易预测的分支可以执行得快得多，因为我们需要扔掉的工作更少，相比一个很难预测的分支。所以让我们看看这个。

这里我们有两个基准测试。它们都做完全相同的事情。我们有一个 `vector` `r`，里面有不同的条目。根据条目，我们要么做乘法，要么做加法。这个 `vector` 是这样生成的。我们做 10 万次迭代。这个 `vector` 是这样生成的。前一半的值是 1。然后后一半的值是 0。所以分支预测器在前半部分会总是预测分支为真。它会发现这一点，然后正确地预测所有，直到我们最终切换到后半部分，那里所有都是 0。它会错误预测一两次。但然后它会发现下一个分支可能又是 0。从那时起，它会再次正确地预测所有。这是我们测试的第一个版本。

第二个版本是，我们也创建了同一个 `vector`，一半 1 一半 0。但在做循环之前，我们只是把它们随机打乱。我不确定为什么这里有两个 `shuffle`，我可以去掉一个。所以我们把它们随机打乱。所以这次 1 和 0 是随机顺序的，分支预测器将很难预测哪个会更快。

根据我们目前的理解，我们预期第一个版本会比第二个版本快得多。对吗？是的。好的。

当我们运行它时，我们看到这确实是真的。所以我们的模型成立。第一个版本比第二个版本快得多。我们可以用这些数字来估算一个错误预测的分支的成本，因为它必须清空整个流水线。所以如果我们计算出在周期数上，我们付出的性能成本是多少，我们就能大致算出我们需要清空的流水线的长度。

所以让我们做这个。这里，我已经填好了时间。它们变化不大。大概是 68000。这是 68000。所以我没作弊。但如果我们执行这个，我们看到，对于可预测的分支，我们每次循环迭代需要的平均周期是两个周期。对于随机分支，是 12 个周期。所以一个错误预测的分支的成本大约是 10 个周期。然而，因为它是随机的，50% 的时间我们可能预测正确了。所以我们可能要把这个乘以 2。所以流水线长度大概在 20 个周期左右。

这显然严重依赖于你运行的 CPU 架构。所有这些基准测试代码，我会在开始时给你们一个 GitHub 链接，所以如果你们想，可以自己玩玩。

好了。这就是分支预测。关于分支预测器有什么问题吗？好的，分支预测器非常强大。它可以发现步长，比如你每隔一次才走分支。是的。

> 观众：“你会谈到间接跳转吗？”  
> 演讲者：“抱歉？”  
> 观众：“在分支预测的背景下，你会谈到间接跳转吗？”  
> 演讲者：“不。好的。你有什么问题？”  
> 观众：“不，不。我没有为那个准备任何东西。不。关于比如去虚拟化和 `switch` 语句的调用等等的讨论，都与分支预测在地址是数据的情况下有关。我想知道是否...谢谢。”  
> 演讲者：“是的。现在，不幸的是，我没有准备那方面的内容。”

好的。这是分支预测。我们目前为止，我们谈了内存地址。我们发现它们不是真实的，因为它们都是虚拟的。我们谈了指令。我们发现它们也不是真实的，因为它们变成了微操作。我们还没真正谈到的最后一件事是寄存器。到目前为止，我们还认为寄存器是真实的。所以让我们看看这个。

我们这里有两个小的汇编片段。这里我们有我们已经做过很多次的同一个 `add` 循环。我选择 4000 次重复的原因是我想确保它能放进 L1 指令缓存。如果我们用 `objdump`，我们可以看到编码这两条指令需要多少字节。是 7 个字节。考虑到 L1 指令缓存的大小和字节数，我们可以计算出 L1 缓存能放多少次迭代或多少条这样的指令。这就是我为什么选 4000。

好的。我们这里有两个例子。第一个只是对 `RCX` 做一个 `add`。第二个是对 `RCX` 做一个 `mov`，然后对 `RCX` 做一个 `add`。所以根据我们目前的理解，我们知道这个大概需要 4000 个周期来执行，因为一个 `add` 需要一个周期。这里我们对 `RCX` 有一个依赖。所以我们假设 `mov` 会在一个周期内完成到 `RCX`，然后 `add` 会在下一个周期完成。所以一个合理的预期可能是第二个版本比第一个版本慢一倍。听起来合理吗？

> 观众：“有反对意见，是的。”  
> 演讲者：“好的。所以你的模型可能比我们目前的模型要先进一点。”  
> 观众：“不，这和寄存器无关。`mov` 对 `add` 没有任何依赖。”  
> 演讲者：“对。好的。我们马上会讲到。但你的直觉完全正确。”

所以让我们试着弄清楚那里发生了什么。

这是时间。第二个版本，即使我们做了两倍的指令，花费的时间却是一半。所以这里发生了什么？

好的。这是那两个代码片段。这个我们之前讨论过，它是一个巨大的依赖链。所以每条指令都依赖于前一条指令的结果。而对于这个，我们需要理解处理器有一个叫做 RAT 的东西。有时它被称为寄存器分配表（Register Allocation Table）。有时是寄存器关联表（Register Association Table）。它叫什么不重要，它叫 RAT。然后它有一个寄存器文件（register file），里面有很多很多的条目。我们这里有 16 个可以引用的条目。而寄存器文件有几百个条目，大概 200 或 300 个。每当我们写入时，我们把指向一个寄存器的指针重定向到寄存器文件中的一个不同的槽位。

所以让我们假设 `RAX` 指向槽位 42。我们把 1 移到 `RAX` 里。所以寄存器文件里的槽位 42 保存着 1。当前，由于某种原因，`RCX` 指向槽位 50。也许值在那里，也许还没在。这不重要。但 `RCX` 当前指向槽位 50。现在，如果我们把某个东西移到 `RCX` 里，我们更新这个映射。所以每当我们写入时，我们更新映射。所以在这条 `mov` 指令之后，`RCX` 将指向槽位 51。所以这个目标指向槽位 51。我们从 `RAX` 移动。我们知道 `RAX` 指向槽位 42。所以我们用槽位 42 作为源。

然后我们 `add`。我们查找源。我们想加到 `RCX` 里。所以我们从 `RCX` 读取并加到 `RCX` 里。所以源当前是槽位 51。我们写入它。所以我们把这个映射更新到槽位 52。并把结果写入槽位 52。然后我们一直这样做。每当我们写入，我们更新槽位。每当我们读取，我们查找当前的槽位。依此类推。

现在，对于依赖关系，对于这个 `add`，它依赖于 S55。我们查找 S55。它依赖于什么？依赖于 S42。依赖于 S42。但我们知道 S42 的值。所以没有依赖。就像这个可以立刻执行。所以这两条指令对之前的东西没有任何依赖。同样。这两条对之前的东西没有任何依赖。所以所有这两对和这两对都可以并行执行。这也可以并行执行。所以后端有更大的自由度来并行执行东西。这就是为什么第二个版本快得多。

好的。然后后端有不同的执行端口。我们早先看到，我们一个周期可以做四个 `add`，这意味着有四个端口支持整数加法。然后调度器就看这个表，检查哪个准备好了，比如哪些依赖已经满足了。然后它就把那些指令推到不同的端口去执行。

到这里，我已经讲完了所有我想讲的内容。这是我运行所有东西的 Haswell 芯片的框图。我们基本上已经涵盖了所有这些，这很酷。所以现在如果你看这个框图，你应该能知道不同模块做什么了。

> 观众：“寄存器关联表会识别寄存器而不是移动或复制吗？”  
> 演讲者：“你说的识别寄存器是什么意思？”  
> 观众：“所以 `mov rcx, rax` 可以简单地把 `RCX` 指向和 `RAX` 相同的槽位。”  
> 演讲者：“是的，这里有一个优化。所以寄存器到寄存器的 `mov` 可能是你能做的除了 `nop` 之外最廉价的事情了。因为它所要做的就是重定向这个箭头。所以这是一个优化。”  
> 观众：“可能会有寄存器指向同一个槽位。”  
> 演讲者：“有可能，是的。是的。”

好的。我们已经谈了所有这些。我们甚至可以验证我们的一些测量...我没法放大，但如果我们放大这里，我们会看到它有四个整数单元。所以那和我们测量的相符。你可以看到这里，我们可以解码并分发四个微操作。所以也和我们的测量相符。

那么所有这些有什么用呢？最近在工作中，我们有一个用例，我们想维护一个排序的 `vector`。它需要支持插入、删除和更新一个元素。我们不想用 `set`，因为我们想以 O(1) 的查找时间查询前 N 个元素。所以我们决定用 `vector`。

有人能想到的最简单的实现就是，如果我们想插入什么东西，我们把新值 `push_back`，然后我们对整个东西排序。如果你在单元测试里这样做，它能工作。如果你在少量数字上做，它可能很快。所以总是在大的输入数量上测试这些东西，来了解性能是很好的。我们看到如果我们插入...如果我们有一个包含一百万个 8 字节整数的 `vector`，我们插入一千次，运行时间是 46 秒。

但现在，这好吗？这坏吗？我们得理解硬件才能对此做出判断。所以让我们看看硬件最少要做什么。

> 观众：“这可能不是个好例子，因为这里的 Big-O 复杂度比一个更好的算法要差。是的。所以这不是硬件问题，是算法问题。”  
> 演讲者：“是的。但为了弄清楚这个算法问题，我们必须理解，硬件最少能做什么？”

所以我们现在来看看。最少，我们要做的是找到我们想插入新元素的位置。我们可以用二分查找来做这个。所以那会是一个更好的算法。一旦我们有了这个位置，我们就可以把它插在这里，这意味着我们只需要把所有后续的元素向后移动。所以我们可以这样做。换句话说，这会是一个 `lower_bound` 加上一个 `insert`。

现在让我们来推断一下硬件应该能多快地执行这个。早先，我们测量了内存带宽。我们知道我们有一百万个 8 字节的元素。所以我们应该在 8MB 这里。所以硬件支持的带宽是 16 GB/s。所以我们，平均来说，我们可能需要移动一半数量的元素。我们有 16GB。然后如果我们做计算，我们预期一次插入应该花大约 230 毫秒。所以我们看到我们差远了。我们做得非常糟糕。

所以让我们把它和 `lower_bound` 加 `insert` 比较，我们得到 204 毫秒。我们比预期的快一点，因为有时我们可能很幸运地命中了 L1 或 L2 缓存。但在做了计算之后，我们知道我们大概在正确的数量级。删除类似，我们可以做一个 `lower_bound` 和 `erase`，运行时间也差不多。对于更新，你可以只删除再插入，它花大概两倍的时间。但再次，如果我们真正思考我们想在内存里做什么改变，而不是只是拼接、删除和插入，这会有帮助。

> 观众：“你可以更快地做所有这些。”  
> 演讲者：“是的。所以如果你思考内存以及我们用删除和插入做什么，我们删除了绿色的那个。然后也许我们想插入一个橙色的。当我们删除绿色时，我们想把绿色更新成橙色。当我们先删除绿色时，我们把这五个元素向左移。然后当我们插入橙色时，我们把三个红色的元素移回它们原来的位置。所以我们在这里做了额外的工作。所以我们可以只这样做，让红色的元素留在原地，然后只交换黄色的和橙色的。”  
> 观众：“你已经看到这个了。算法是什么？”  
> 演讲者：“`rotate`。是的。所以 `rotate`。所以我们只需要做两个 `lower_bound` 加一个 `rotate`。如果我们那样做，我们得到的执行时间是 141 毫秒。”  
> 观众：“这不...你说你把这个放在 `vector` 里而不是 `set` 里，因为你想要 O(1) 的前 N 个。N 是个固定的数字还是每次都不同？”  
> 演讲者：“它可能不同。是的。”

好的。这是一个 C++ 的例子。我认为不仅在 C++ 中我们必须把内存和时间消耗记在心里。这里有一个小的 Python 例子。想象一下你的任务是把所有的数量加起来。所以当你开发你的小代码时，你遍历所有东西。你得到欠的数量。你做这个列表推导式。为了调试，你打印出数量，只是为了确保它合理。然后你应该把所有数量加起来，所以你把它们都加起来。完成了。你用几个元素测试它。内存消耗是 9MB，这可能只是解释器本身。因为这不应该花那么多，但这还包括了解释器。但然后在生产环境中，你有很多元素。突然你的内存消耗是 400MB。

现在，如果你了解硬件，并且你知道加法可以在一个寄存器内完成，那么为了做这个求和需要那么多内存是没有道理的。所以你所要做的就是，不用列表推导式，用一个生成器表达式，你的内存就降下来了。所以总是在大量输入上测试，并且大致推断一下这是否是预期的行为，这总是有帮助的。如果不是，要么改进它，要么理解为什么你的预期是错的。

好的。到这里，我们的旅程就结束了。我们从左边一直走到了右边。如果你想玩玩这个，这里有一个 GitHub 仓库，包含了今天我用于基准测试的所有代码。下面是我使用的所有工具的概览。这个仓库用了 B-Man 模板，这对于设置所有 CMake 的东西非常有帮助。

就是这样。感谢大家的聆听。

---

**问答环节 (Q&A)**

> **观众：** 这一切显然意味着，对于算法开发来说，我们正处于一个非常痛苦的世界，如果我没理解错的话。对吧？因为根据你用的处理器不同，你探讨的所有变量都是不同的。  
> **演讲者：** 对。是的。所以，你知道，你设计的任何算法可能在这组处理器上表现超好，而在那边却很糟糕。对。是的。我会说，你肯定可以...如果你真的想为某个特定的处理器把它调到极致，那就得再做些实验。比如，如果你一个周期能做五个加法，你可以从那里反推。理论上，我能做那么多，然后你可以看你的算法，把一切都做到极致。我同意你，那不是一个可行的方法，因为有太多的处理器，而且新的总是在出来。但我确实认为，可能有一个很好的权衡，对于各种不同的东西都能有合理的性能。

> **观众：** 但，你知道，另一部分是你甚至还没探讨 SIMD 的途径。是的。这意味着，这里还有另一个复杂度的维度。所以甚至在你谈论，是的，是关于算法来做排序之类的之前。但是，你知道，你怎么比较两个算法，如果这个可以并行运行，而那个不知何故不能。  
> **演讲者：** 是的。是的。我完全同意你。然后还有显卡之类的。所以很难为所有可能运行的硬件进行优化。我并不完全同意你...大多数硬件就是你描述的这样。这是 95% 的现代桌面和服务器 CPU。比如，对于 SIMD，一些 CPU 有 512 位宽的寄存器，一些则没有。是的，但没有 CPU 没有 L1 缓存。没有 CPU...是的。而且，在大量元素上，复杂度更低的算法总是会运行得更好。所以即使我们把它拿到一个完全不同的 CPU 上，所以了解这些参数非常有用，但对于大多数编程任务，优化是在更高的层次上，而且...  
> **演讲者：** 我的意思是，如果你的算法本身就是次优的，你不应该从优化汇编开始。  
> **观众：** 之前很早的时候，所以...我可能忘了细节了，但你提到了一些你遇到的性能影响，是来自于...页...页...页面，对吧？所以，你有没有探索过使用巨页（huge pages）？那显然会有帮助。  
> **演讲者：** 如果你不必每 4KB 就发生一次缺页中断，而是每 GB 才一次，那肯定有帮助。所以如果你知道缺页中断，并且你知道那是瓶颈，那你就可以做些什么。所以重要的事情是，有工具去观察它，并且理解它如何工作。  

> **观众：** 你左边用的那个工具，左边是源代码，右边是汇编。那是什么工具？  
> **演讲者：** 那是我自己的 Vim 插件，但它背后用的是 Compiler Explorer。  
> **观众：** 真的？那个扩展叫什么名字？  
> **演讲者：** 它就在我的个人 GitHub 上。啊，这里有吗？我想它可能是公开的。所以在 GitHub 上你可能能看到它。  

> **观众：** 一个非常重要的问题。你用的是 Linux，但上面有个 Windows 的 logo...  
> **演讲者：** 所以我...我登录到了...所以，是的，我不太擅长 Windows。所以我在 Windows 上做不了任何这些。我装了 WSL，但这些在 WSL 里都跑不起来。所以我 SSH 到了一个 Linux 机器上来运行所有东西。

好了，我想我们的时间到了。感谢大家的聆听。谢谢。非常感谢。谢谢大家。
